{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae7bc019-eb64-4fda-b5c0-c5f2eb75cecd",
   "metadata": {},
   "source": [
    "## ‚úÖ 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78942cf1-3066-4b36-9aad-ef99df737cd5",
   "metadata": {},
   "source": [
    "### ‚úÖ Climate Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc594523-353f-4fb1-9d90-a781a1544d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# === Step 0: Load climate data ===\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# --- 1. Parse dates and extract YEAR/MONTH ---\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "climate_df['MONTH'] = climate_df['DATE'].dt.month\n",
    "\n",
    "# --- 2. Aggregate yearly climate indicators by district ---\n",
    "climate_yearly = (\n",
    "    climate_df\n",
    "    .dropna(subset=['DISTRICT', 'YEAR'])  # ensure no missing grouping keys\n",
    "    .groupby(['DISTRICT', 'YEAR'], as_index=False)\n",
    "    .agg({\n",
    "        'T2M': 'mean',                # Avg temp\n",
    "        'T2M_MAX': 'mean',            # Avg max temp\n",
    "        'T2M_RANGE': 'std',           # Std dev of temp range\n",
    "        'PRECTOT': 'sum',             # Total precipitation\n",
    "        'RH2M': 'mean',               # Avg humidity\n",
    "        'WS10M': 'mean'               # Avg wind\n",
    "    })\n",
    "    .rename(columns={\n",
    "        'T2M': 'avg_temp',\n",
    "        'T2M_MAX': 'avg_max_temp',\n",
    "        'T2M_RANGE': 'temp_range_stddev',\n",
    "        'PRECTOT': 'annual_precip',\n",
    "        'RH2M': 'avg_humidity',\n",
    "        'WS10M': 'avg_wind'\n",
    "    })\n",
    ")\n",
    "\n",
    "# --- 3. Count heatwave days (T2M_MAX > 35¬∞C) per district-year ---\n",
    "heatwave_thresh = 35\n",
    "heatwave_count = (\n",
    "    climate_df[climate_df['T2M_MAX'] > heatwave_thresh]\n",
    "    .groupby(['DISTRICT', 'YEAR'])\n",
    "    .size()\n",
    "    .reset_index(name='heatwave_days')\n",
    ")\n",
    "\n",
    "# --- 4. Merge heatwave counts into climate summary ---\n",
    "climate_yearly = climate_yearly.merge(heatwave_count, on=['DISTRICT', 'YEAR'], how='left')\n",
    "climate_yearly['heatwave_days'] = climate_yearly['heatwave_days'].fillna(0).astype(int)\n",
    "\n",
    "# --- 5. Compute Z-score of annual precipitation within each district ---\n",
    "climate_yearly['precip_zscore'] = (\n",
    "    climate_yearly.groupby('DISTRICT')['annual_precip']\n",
    "    .transform(lambda x: zscore(x, ddof=0))\n",
    ")\n",
    "\n",
    "# --- 6. Create 1-year lag features for selected indicators ---\n",
    "lag_features = ['avg_temp', 'annual_precip', 'precip_zscore', 'heatwave_days', 'temp_range_stddev']\n",
    "for col in lag_features:\n",
    "    climate_yearly[f'{col}_lag1'] = climate_yearly.groupby('DISTRICT')[col].shift(1)\n",
    "\n",
    "# --- 7. Drop rows with any missing lag values ---\n",
    "climate_yearly = climate_yearly.dropna().reset_index(drop=True)\n",
    "\n",
    "# --- 8. Preview result ---\n",
    "print(f\"‚úÖ Climate Feature Set: {climate_yearly.shape[0]} rows √ó {climate_yearly.shape[1]} columns\")\n",
    "print(climate_yearly.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f98cb80-3f68-4eef-9afe-2f1942297ac0",
   "metadata": {},
   "source": [
    "#### ‚úÖ Heatwave Days Index (District-Year Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9079e-f936-4d68-8504-49e9ed606d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 0: Load data (if not already loaded)\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# Step 1: Parse DATE and extract YEAR\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "\n",
    "# Step 2: Define heatwave threshold (e.g., T2M_MAX > 35¬∞C)\n",
    "heatwave_threshold = 35  # degrees Celsius\n",
    "\n",
    "# Step 3: Filter for extreme heat days\n",
    "heatwave_df = climate_df[climate_df['T2M_MAX'] > heatwave_threshold].copy()\n",
    "\n",
    "# Step 4: Count number of heatwave days per district-year\n",
    "heatwave_index = (\n",
    "    heatwave_df\n",
    "    .dropna(subset=['DISTRICT', 'YEAR'])  # ensure grouping keys are valid\n",
    "    .groupby(['DISTRICT', 'YEAR'])\n",
    "    .size()\n",
    "    .reset_index(name='heatwave_days')\n",
    ")\n",
    "\n",
    "# Step 5: Preview result\n",
    "print(\"‚úÖ Heatwave Days Index (District-Year Level):\")\n",
    "print(heatwave_index.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d377c16-3779-4022-9b9f-73d6e1214b01",
   "metadata": {},
   "source": [
    "#### ‚úÖ Simple Precipitation Index (SPI Proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc1bb3a-af76-4e69-8dca-635da08f0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Step 0: Load data if not already defined\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# Step 1: Ensure DATE is datetime and extract YEAR\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "\n",
    "# Step 2: Aggregate annual total precipitation per district\n",
    "annual_precip = (\n",
    "    climate_df\n",
    "    .dropna(subset=['DISTRICT', 'YEAR', 'PRECTOT'])\n",
    "    .groupby(['DISTRICT', 'YEAR'], as_index=False)['PRECTOT']\n",
    "    .sum()\n",
    "    .rename(columns={'PRECTOT': 'annual_precip'})\n",
    ")\n",
    "\n",
    "# Step 3: Compute Z-score (SPI proxy) per district\n",
    "# If a district has only one year of data, assign 0 to avoid NaNs\n",
    "def safe_zscore(series):\n",
    "    return zscore(series, ddof=0) if series.nunique() > 1 else pd.Series([0]*len(series), index=series.index)\n",
    "\n",
    "annual_precip['precip_zscore'] = (\n",
    "    annual_precip.groupby('DISTRICT')['annual_precip']\n",
    "    .transform(safe_zscore)\n",
    ")\n",
    "\n",
    "# Step 4: Preview the result\n",
    "print(\"‚úÖ SPI Proxy (Z-score of Annual Precipitation):\")\n",
    "print(annual_precip.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85454b2-7b41-4c4f-a049-bd8708cc4d42",
   "metadata": {},
   "source": [
    "#### ‚úÖ Monsoon Onset Day Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983bce55-85cf-4757-ba26-01e9250afbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 0: Load data if not already available\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# Step 1: Parse dates and extract date parts\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "climate_df['MONTH'] = climate_df['DATE'].dt.month\n",
    "climate_df['DAY'] = climate_df['DATE'].dt.day\n",
    "\n",
    "# Step 2: Filter for monsoon months (June‚ÄìSeptember = JJAS)\n",
    "monsoon_df = climate_df[climate_df['MONTH'].isin([6, 7, 8, 9])].copy()\n",
    "\n",
    "# Step 3: Identify rainy days (daily precipitation > 10 mm)\n",
    "rainy_days = monsoon_df[monsoon_df['PRECTOT'] > 10].copy()\n",
    "\n",
    "# Step 4: Compute earliest rainy day as monsoon onset per district-year\n",
    "onset_day = (\n",
    "    rainy_days\n",
    "    .dropna(subset=['DISTRICT', 'YEAR', 'DATE'])\n",
    "    .groupby(['DISTRICT', 'YEAR'], as_index=False)['DATE']\n",
    "    .min()\n",
    "    .rename(columns={'DATE': 'monsoon_onset_date'})\n",
    ")\n",
    "\n",
    "# Step 5: Extract day of month from onset date (optional)\n",
    "onset_day['onset_day'] = onset_day['monsoon_onset_date'].dt.day\n",
    "\n",
    "# Step 6: Preview result\n",
    "print(\"‚úÖ Monsoon Onset Day per District-Year:\")\n",
    "print(onset_day.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804bfe5-ad4a-48c4-901b-36879ea5491c",
   "metadata": {},
   "source": [
    "#### ‚úÖ Annual Temperature Variability per District-Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a33dee-c20e-4ff5-8185-7e58026ee074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 0: Load data (if not already done)\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# Step 1: Ensure DATE is datetime and extract YEAR\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "\n",
    "# Step 2: Drop rows with missing DISTRICT, YEAR, or T2M_RANGE\n",
    "climate_clean = climate_df.dropna(subset=['DISTRICT', 'YEAR', 'T2M_RANGE']).copy()\n",
    "\n",
    "# Step 3: Compute standard deviation of daily temperature range per district-year\n",
    "tempvar_df = (\n",
    "    climate_clean\n",
    "    .groupby(['DISTRICT', 'YEAR'], as_index=False)['T2M_RANGE']\n",
    "    .std()\n",
    "    .rename(columns={'T2M_RANGE': 'temp_range_stddev'})\n",
    ")\n",
    "\n",
    "# Step 4: Preview results\n",
    "print(\"‚úÖ Annual Temperature Variability (Std Dev of T2M_RANGE):\")\n",
    "print(tempvar_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5a967-4808-47e0-8520-8c72007ddf94",
   "metadata": {},
   "source": [
    "#### ‚úÖ Total Monsoon Rainfall per District-Year (June‚ÄìSeptember)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856eca40-8d73-4391-9f85-7fb218db2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 0: Load data if not already defined\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# Step 1: Convert DATE to datetime and extract YEAR and MONTH\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "climate_df['MONTH'] = climate_df['DATE'].dt.month\n",
    "\n",
    "# Step 2: Filter for monsoon months (June‚ÄìSeptember)\n",
    "monsoon_df = climate_df[climate_df['MONTH'].isin([6, 7, 8, 9])].copy()\n",
    "\n",
    "# Step 3: Aggregate total monsoon precipitation per district-year\n",
    "monsoon_rainfall = (\n",
    "    monsoon_df\n",
    "    .dropna(subset=['DISTRICT', 'YEAR', 'PRECTOT'])\n",
    "    .groupby(['DISTRICT', 'YEAR'], as_index=False)['PRECTOT']\n",
    "    .sum(min_count=1)\n",
    "    .rename(columns={'PRECTOT': 'monsoon_precip'})\n",
    ")\n",
    "\n",
    "# Step 4: Preview result\n",
    "print(\"‚úÖ Total Monsoon Rainfall (June‚ÄìSeptember) per District-Year:\")\n",
    "print(monsoon_rainfall.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bec5cf2-b81c-42c1-b2a8-32c14cbf8949",
   "metadata": {},
   "source": [
    "#### ‚úÖ Dry Spell Days Detection During Monsoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1723c1f-adc6-476c-9129-85cee8bc09b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 0: Load dataset if not already loaded\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# Step 1: Parse dates and extract YEAR and MONTH\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "climate_df['MONTH'] = climate_df['DATE'].dt.month\n",
    "\n",
    "# Step 2: Filter for monsoon months (June‚ÄìSeptember)\n",
    "monsoon_df = climate_df[climate_df['MONTH'].isin([6, 7, 8, 9])].copy()\n",
    "\n",
    "# Step 3: Identify dry days (daily precipitation < 1 mm)\n",
    "dry_days_df = monsoon_df[monsoon_df['PRECTOT'] < 1].copy()\n",
    "\n",
    "# Step 4: Count dry spell days per district-year\n",
    "dry_spell_count = (\n",
    "    dry_days_df\n",
    "    .dropna(subset=['DISTRICT', 'YEAR'])  # Ensure no null group keys\n",
    "    .groupby(['DISTRICT', 'YEAR'])\n",
    "    .size()\n",
    "    .reset_index(name='dry_spell_days')\n",
    ")\n",
    "\n",
    "# Step 5: Preview result\n",
    "print(\"‚úÖ Dry Spell Days During Monsoon per District-Year:\")\n",
    "print(dry_spell_count.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d53c74-d2bb-47d5-905e-8a3bff8d99e3",
   "metadata": {},
   "source": [
    "### ‚úÖ Generate lag features (e.g., prior year heatwave or rainfall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c098894-56fa-4bfb-beb1-129aa8bbc53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- 1. Merge annual precipitation with heatwave index ---\n",
    "features_df = pd.merge(annual_precip, heatwave_index, on=['DISTRICT', 'YEAR'], how='left')\n",
    "\n",
    "# If precip_zscore doesn't exist, compute it now\n",
    "if 'precip_zscore' not in features_df.columns:\n",
    "    features_df['precip_zscore'] = (\n",
    "        features_df.groupby('DISTRICT')['annual_precip']\n",
    "        .transform(lambda x: (x - x.mean()) / x.std(ddof=0))\n",
    "    )\n",
    "\n",
    "# --- 2. Merge monsoon onset day ---\n",
    "features_df = pd.merge(\n",
    "    features_df,\n",
    "    onset_day[['DISTRICT', 'YEAR', 'onset_day']],\n",
    "    on=['DISTRICT', 'YEAR'],\n",
    "    how='left'\n",
    ").rename(columns={'onset_day': 'monsoon_onset_day'})\n",
    "\n",
    "# --- 3. Merge temperature range variability ---\n",
    "features_df = pd.merge(features_df, tempvar_df, on=['DISTRICT', 'YEAR'], how='left')\n",
    "\n",
    "# --- 4. Fill missing heatwave days with 0 (no events recorded) ---\n",
    "features_df['heatwave_days'] = features_df['heatwave_days'].fillna(0).astype(int)\n",
    "\n",
    "# --- 5. Sort data for time-aware lagging ---\n",
    "features_df = features_df.sort_values(by=['DISTRICT', 'YEAR']).copy()\n",
    "\n",
    "# --- 6. Create 1-year lag features ---\n",
    "lag_vars = [\n",
    "    'heatwave_days',\n",
    "    'annual_precip',\n",
    "    'precip_zscore',\n",
    "    'temp_range_stddev',\n",
    "    'monsoon_onset_day'\n",
    "]\n",
    "\n",
    "for var in lag_vars:\n",
    "    features_df[f'{var}_lag1'] = features_df.groupby('DISTRICT')[var].shift(1)\n",
    "\n",
    "# --- 7. Drop rows with missing lag values (usually first year per district) ---\n",
    "features_df = features_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# --- 8. Preview final output ---\n",
    "print(f\"‚úÖ Engineered Feature Set (with Lags): {features_df.shape[0]} rows √ó {features_df.shape[1]} columns\")\n",
    "print(features_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc502f05-f740-4e06-8ea2-06201915b632",
   "metadata": {},
   "source": [
    "### ‚úÖ Cereal Yield Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f1392-e885-4eeb-b7bc-a91da7631719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Step 1: Load the dataset ===\n",
    "agri_path = \"../data/nepal_agri_stats_cereal_197980_201314.csv\"\n",
    "agri_df = pd.read_csv(agri_path)\n",
    "\n",
    "print(\"üìÑ Raw Cereal Yield Data Info:\")\n",
    "agri_df.info()\n",
    "print(\"\\nüîç Preview:\")\n",
    "print(agri_df.head())\n",
    "\n",
    "# === Step 2: Clean column names ===\n",
    "agri_df.columns = agri_df.columns.str.strip().str.upper()\n",
    "\n",
    "# === Step 3: Identify yield columns ===\n",
    "yield_cols = [col for col in agri_df.columns if '_Y_' in col]\n",
    "\n",
    "# === Step 4: Subset and reshape ===\n",
    "yield_df = agri_df[['DISTRICT_NAME'] + yield_cols].copy()\n",
    "yield_long = yield_df.melt(\n",
    "    id_vars='DISTRICT_NAME',\n",
    "    var_name='CROP_FY',\n",
    "    value_name='YIELD'\n",
    ")\n",
    "\n",
    "# === Step 5: Extract crop and fiscal year ===\n",
    "extracted = yield_long['CROP_FY'].str.extract(r'([A-Z]+)_Y_(\\d{6})')\n",
    "yield_long['CROP'] = extracted[0].str.lower()\n",
    "yield_long['FY'] = extracted[1].apply(lambda x: f\"{x[:4]}/{x[4:]}\" if pd.notna(x) else None)\n",
    "\n",
    "# === Step 6: Drop incomplete entries and tidy ===\n",
    "yield_long.dropna(subset=['DISTRICT_NAME', 'CROP', 'FY', 'YIELD'], inplace=True)\n",
    "yield_long = yield_long[['DISTRICT_NAME', 'CROP', 'FY', 'YIELD']]\n",
    "\n",
    "print(\"\\n‚úÖ Tidy Yield Data Preview:\")\n",
    "print(yield_long.head())\n",
    "\n",
    "# === Step 7: Pivot to wide format (district-year with separate columns per crop) ===\n",
    "yield_wide = yield_long.pivot_table(\n",
    "    index=['DISTRICT_NAME', 'FY'],\n",
    "    columns='CROP',\n",
    "    values='YIELD',\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "yield_wide.columns.name = None\n",
    "yield_wide.columns = [col.lower() if isinstance(col, str) else col for col in yield_wide.columns]\n",
    "\n",
    "# === Step 8: Extract numeric year from FY ===\n",
    "yield_wide['year'] = yield_wide['fy'].str.extract(r'(\\d{4})')[0].astype(int)\n",
    "\n",
    "# === Step 9: Engineer crop features ===\n",
    "yield_cols = ['mz', 'pd', 'wt']  # maize, paddy, wheat\n",
    "\n",
    "# Check if all crops are present\n",
    "missing = [col for col in yield_cols if col not in yield_wide.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"‚ùå Missing expected crop columns: {missing}\")\n",
    "\n",
    "# Total cereal yield\n",
    "yield_wide['total_yield'] = yield_wide[yield_cols].sum(axis=1)\n",
    "\n",
    "# Year-over-year yield change\n",
    "for crop in yield_cols:\n",
    "    yield_wide[f'{crop}_change'] = yield_wide.groupby('district_name')[crop].diff()\n",
    "\n",
    "# 3-year rolling mean and std\n",
    "for crop in yield_cols:\n",
    "    yield_wide[f'{crop}_ma3'] = (\n",
    "        yield_wide.groupby('district_name')[crop]\n",
    "        .transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "    )\n",
    "    yield_wide[f'{crop}_std3'] = (\n",
    "        yield_wide.groupby('district_name')[crop]\n",
    "        .transform(lambda x: x.rolling(window=3, min_periods=1).std())\n",
    "    )\n",
    "\n",
    "# Lag features\n",
    "for crop in yield_cols:\n",
    "    yield_wide[f'{crop}_lag1'] = yield_wide.groupby('district_name')[crop].shift(1)\n",
    "\n",
    "# === Step 10: Final cleanup ===\n",
    "yield_features = yield_wide.drop(columns=['fy']).copy()\n",
    "\n",
    "# === Step 11: Preview output ===\n",
    "print(f\"\\n‚úÖ Cereal Yield Feature Set: {yield_features.shape[0]} rows √ó {yield_features.shape[1]} columns\")\n",
    "print(yield_features.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e1a52f-f88b-4ed0-873f-17be2b8423cd",
   "metadata": {},
   "source": [
    "### ‚úÖ Land Use Feature Engineering (1967-2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f4122d-0923-4132-9bc7-db9c01ebc5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# Step 1: Load land use data\n",
    "land_use_path = '../data/land_use_statistics_1967_2010.csv'\n",
    "land_use_df = pd.read_csv(land_use_path)\n",
    "\n",
    "# Step 2: Clean column names\n",
    "land_use_df.columns = (\n",
    "    land_use_df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(' ', '_')\n",
    "    .str.replace('%', '', regex=False)\n",
    ")\n",
    "\n",
    "# Step 3: Preview structure\n",
    "print(\"üìÑ Land Use Data Info:\")\n",
    "land_use_df.info()\n",
    "print(\"\\nüîç Land Use Data Preview:\")\n",
    "print(land_use_df.head())\n",
    "\n",
    "# Step 4: Melt to long format\n",
    "land_use_long = land_use_df.melt(\n",
    "    id_vars='land_use_type',\n",
    "    var_name='year',\n",
    "    value_name='percentage'\n",
    ")\n",
    "\n",
    "# Step 5: Extract numeric year\n",
    "land_use_long['year'] = pd.to_numeric(\n",
    "    land_use_long['year'].str.extract(r'(\\d{4})')[0],\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Step 6: Standardize land use type names\n",
    "land_use_long['land_use_type'] = (\n",
    "    land_use_long['land_use_type']\n",
    "    .str.lower()\n",
    "    .str.replace(r'[^a-z\\s]', '', regex=True)  # Remove non-letter characters\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Step 7: Drop rows with missing values\n",
    "land_use_long.dropna(subset=['year', 'percentage'], inplace=True)\n",
    "\n",
    "# Step 8: Preview cleaned long-format data\n",
    "print(\"\\nüìä Tidy Land Use Data (Long Format):\")\n",
    "print(land_use_long.head())\n",
    "\n",
    "# Step 9: Define categories of interest and engineer features\n",
    "target_types = ['urban', 'agriculture', 'shrub', 'water']\n",
    "land_use_features = []\n",
    "\n",
    "for use_type in target_types:\n",
    "    df = land_use_long[land_use_long['land_use_type'].str.contains(use_type)].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        print(f\"‚ö†Ô∏è No records found for land use type: {use_type}\")\n",
    "        continue\n",
    "\n",
    "    df = df.sort_values('year').reset_index(drop=True)\n",
    "    df.rename(columns={'percentage': f'{use_type}_percent'}, inplace=True)\n",
    "\n",
    "    # Change metrics\n",
    "    df[f'{use_type}_change_abs'] = df[f'{use_type}_percent'].diff()\n",
    "    df[f'{use_type}_change_pct'] = df[f'{use_type}_percent'].pct_change() * 100\n",
    "\n",
    "    # 3-year rolling metrics\n",
    "    df[f'{use_type}_ma3'] = df[f'{use_type}_percent'].rolling(window=3, min_periods=1).mean()\n",
    "    df[f'{use_type}_std3'] = df[f'{use_type}_percent'].rolling(window=3, min_periods=1).std()\n",
    "\n",
    "    # Select only relevant columns\n",
    "    land_use_features.append(\n",
    "        df[['year', f'{use_type}_percent', f'{use_type}_change_abs',\n",
    "            f'{use_type}_change_pct', f'{use_type}_ma3', f'{use_type}_std3']]\n",
    "    )\n",
    "\n",
    "# Step 10: Merge all feature tables on 'year'\n",
    "if land_use_features:\n",
    "    land_use_merged = reduce(\n",
    "        lambda left, right: pd.merge(left, right, on='year', how='outer'),\n",
    "        land_use_features\n",
    "    ).sort_values('year').reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\n‚úÖ Land Use Feature Set Ready: {land_use_merged.shape[0]} rows √ó {land_use_merged.shape[1]} columns\")\n",
    "    print(land_use_merged.head())\n",
    "else:\n",
    "    print(\"‚ùå No land use features generated. Please check the category names.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8db2bb0-e4a6-4c30-9475-5f50ddc4b828",
   "metadata": {},
   "source": [
    "### ‚úÖ Glacier Retreat Feature Engineering (1980‚Äì2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986175f-4bc8-456e-8a0f-d86b3793f63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Load glacier data ---\n",
    "glacier_path = '../data/glaciers_change_in_basins_subbasins_1980_1990_2000_2010.csv'\n",
    "glacier_df = pd.read_csv(glacier_path)\n",
    "\n",
    "# --- Step 2: Standardize column names ---\n",
    "glacier_df.columns = (\n",
    "    glacier_df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace('~', '', regex=False)\n",
    "    .str.replace(' ', '_')\n",
    "    .str.replace(r'\\(km2\\)', '', regex=True)\n",
    "    .str.replace(r'\\(km3\\)', '', regex=True)\n",
    "    .str.replace(r'\\(masl\\)', '', regex=True)\n",
    "    .str.replace(r'[()]', '', regex=True)\n",
    ")\n",
    "\n",
    "# --- Step 3: Rename columns for reshaping ---\n",
    "glacier_df.rename(columns={\n",
    "    'glacier_no._in_1980': 'glacier_count_1980',\n",
    "    'glacier_no._in_1990': 'glacier_count_1990',\n",
    "    'glacier_no._in_2000': 'glacier_count_2000',\n",
    "    'glacier_no._in_2010': 'glacier_count_2010',\n",
    "    'glacier_area_in_1980': 'glacier_area_1980',\n",
    "    'glacier_area_1990': 'glacier_area_1990',\n",
    "    'glacier_area_2000': 'glacier_area_2000',\n",
    "    'glacier_area_2010': 'glacier_area_2010',\n",
    "    'estimated_ice_reserved_1980': 'ice_volume_1980',\n",
    "    'estimated_ice_reserved_1990': 'ice_volume_1990',\n",
    "    'estimated_ice_reserved2000': 'ice_volume_2000',\n",
    "    'estimated_ice_reserved2010': 'ice_volume_2010',\n",
    "    'minimum_elevation_in_1980': 'min_elev_1980',\n",
    "    'minimum_elevation_in1990': 'min_elev_1990',\n",
    "    'minimum_elevation_in2000': 'min_elev_2000',\n",
    "    'minimum_elevation_in2010': 'min_elev_2010'\n",
    "}, inplace=True)\n",
    "\n",
    "# --- Step 4: Reshape to long format ---\n",
    "glacier_long = pd.wide_to_long(\n",
    "    glacier_df,\n",
    "    stubnames=['glacier_count', 'glacier_area', 'ice_volume', 'min_elev'],\n",
    "    i=['basin', 'sub-basin'],\n",
    "    j='year',\n",
    "    sep='_',\n",
    "    suffix='(1980|1990|2000|2010)'\n",
    ").reset_index()\n",
    "\n",
    "glacier_long['year'] = glacier_long['year'].astype(int)\n",
    "\n",
    "# --- Step 5: Pivot to compare 1980 vs 2010 side-by-side ---\n",
    "pivoted = (\n",
    "    glacier_long[glacier_long['year'].isin([1980, 2010])]\n",
    "    .pivot(index=['basin', 'sub-basin'], columns='year', values=['glacier_area', 'ice_volume', 'min_elev'])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "pivoted.columns = [\n",
    "    f\"{var}_{int(yr)}\" if isinstance(yr, int) else var\n",
    "    for var, yr in pivoted.columns.to_flat_index()\n",
    "]\n",
    "\n",
    "# --- Step 6: Calculate losses and per-decade changes ---\n",
    "pivoted['area_loss_km2'] = pivoted['glacier_area_2010'] - pivoted['glacier_area_1980']\n",
    "pivoted['area_loss_pct'] = (pivoted['area_loss_km2'] / pivoted['glacier_area_1980']) * 100\n",
    "\n",
    "pivoted['volume_loss_km3'] = pivoted['ice_volume_1980'] - pivoted['ice_volume_2010']\n",
    "pivoted['volume_loss_pct'] = (pivoted['volume_loss_km3'] / pivoted['ice_volume_1980']) * 100\n",
    "\n",
    "pivoted['elev_rise_m'] = pivoted['min_elev_2010'] - pivoted['min_elev_1980']\n",
    "\n",
    "pivoted['area_loss_pct_per_decade'] = pivoted['area_loss_pct'] / 3\n",
    "pivoted['volume_loss_pct_per_decade'] = pivoted['volume_loss_pct'] / 3\n",
    "pivoted['elev_rise_per_decade'] = pivoted['elev_rise_m'] / 3\n",
    "\n",
    "# --- Step 7: Classify retreat severity ---\n",
    "def classify_severity(pct):\n",
    "    if pct <= -50:\n",
    "        return 'High'\n",
    "    elif pct <= -25:\n",
    "        return 'Moderate'\n",
    "    elif pct < 0:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'No Loss or Gain'\n",
    "\n",
    "pivoted['retreat_severity'] = pivoted['area_loss_pct'].apply(classify_severity)\n",
    "\n",
    "# --- Step 8: Select final columns ---\n",
    "glacier_features = pivoted[[\n",
    "    'basin', 'sub-basin',\n",
    "    'glacier_area_1980', 'glacier_area_2010', 'area_loss_km2', 'area_loss_pct', 'area_loss_pct_per_decade',\n",
    "    'ice_volume_1980', 'ice_volume_2010', 'volume_loss_km3', 'volume_loss_pct', 'volume_loss_pct_per_decade',\n",
    "    'min_elev_1980', 'min_elev_2010', 'elev_rise_m', 'elev_rise_per_decade',\n",
    "    'retreat_severity'\n",
    "]].copy()\n",
    "\n",
    "# --- Step 9: Output ---\n",
    "print(f\"‚úÖ Glacier Retreat Feature Set Ready: {glacier_features.shape[0]} rows √ó {glacier_features.shape[1]} columns\")\n",
    "print(glacier_features.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48fbda3-6ac1-4956-bfa1-9581bd835b92",
   "metadata": {},
   "source": [
    "### ‚úÖ Merge Climate and Yield Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46218843-c49e-44c5-ba4c-7624584ca2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Normalize district names for safe merge\n",
    "climate_yearly['DISTRICT'] = climate_yearly['DISTRICT'].str.strip().str.lower()\n",
    "yield_features['district_name'] = yield_features['district_name'].str.strip().str.lower()\n",
    "\n",
    "# Step 2: Merge climate and yield datasets on district name and year\n",
    "merged_df = pd.merge(\n",
    "    yield_features,\n",
    "    climate_yearly,\n",
    "    left_on=['district_name', 'year'],\n",
    "    right_on=['DISTRICT', 'YEAR'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Step 3: Drop redundant keys from the right dataframe\n",
    "merged_df.drop(columns=['DISTRICT', 'YEAR'], inplace=True)\n",
    "\n",
    "# Step 4: Preview merged dataset\n",
    "print(f\"‚úÖ Merged Dataset: {merged_df.shape[0]} rows √ó {merged_df.shape[1]} columns\")\n",
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c515fc7-f4e3-4931-9224-f21085071227",
   "metadata": {},
   "source": [
    "### ‚úÖ Correlation between climate and yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a959c7b-f798-4bad-bbad-18957291af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check for required columns\n",
    "cols_to_check = ['total_yield', 'avg_temp', 'annual_precip', 'heatwave_days']\n",
    "missing_cols = [col for col in cols_to_check if col not in merged_df.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"‚ùå Missing expected columns: {missing_cols}\")\n",
    "\n",
    "# Step 2: Compute correlation matrix\n",
    "correlation = merged_df[cols_to_check].dropna().corr()\n",
    "\n",
    "# Step 3: Display correlation matrix\n",
    "print(\"‚úÖ Correlation Matrix (Yield vs. Climate Variables):\")\n",
    "print(correlation.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd39adf-a7fd-48c9-aaa7-af7b1df7bfb8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Glacier Area Loss by Sub-Basin (1980‚Äì2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ca5611-f107-4262-a738-38f32993c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Ensure area_loss_pct is numeric\n",
    "glacier_features['area_loss_pct'] = pd.to_numeric(\n",
    "    glacier_features['area_loss_pct'], errors='coerce'\n",
    ")\n",
    "\n",
    "# Step 2: Sort by loss % for ordered plotting\n",
    "plot_df = glacier_features.sort_values('area_loss_pct', ascending=True)\n",
    "\n",
    "# Step 3: Define color palette for severity levels\n",
    "severity_palette = {\n",
    "    'High': '#d73027',          # Red\n",
    "    'Moderate': '#fc8d59',      # Orange\n",
    "    'Low': '#fee08b',           # Yellow\n",
    "    'No Loss or Gain': '#91bfdb'  # Blue\n",
    "}\n",
    "\n",
    "# Step 4: Create horizontal bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=plot_df,\n",
    "    x='area_loss_pct',\n",
    "    y='sub-basin',\n",
    "    hue='retreat_severity',\n",
    "    palette=severity_palette,\n",
    "    dodge=False\n",
    ")\n",
    "\n",
    "# Step 5: Styling\n",
    "plt.axvline(0, color='black', linewidth=0.8)\n",
    "plt.title('Glacier Area Loss by Sub-Basin (1980‚Äì2010)', fontsize=14, weight='bold')\n",
    "plt.xlabel('Area Loss (%)', fontsize=12)\n",
    "plt.ylabel('Sub-Basin', fontsize=12)\n",
    "plt.legend(title='Retreat Severity', frameon=False)\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbabebc-17b7-446d-b4ca-d665fd3e1130",
   "metadata": {},
   "source": [
    "### Stacked Bar Plot of Glacier Area Lost vs. Remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c68f6d-a301-4f5b-baeb-09a996c86bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Prepare glacier area components\n",
    "glacier_stack = glacier_features.copy()\n",
    "glacier_stack['area_remaining'] = glacier_stack['glacier_area_2010']\n",
    "glacier_stack['area_lost'] = (\n",
    "    glacier_stack['glacier_area_1980'] - glacier_stack['glacier_area_2010']\n",
    ").clip(lower=0)  # Avoid negative values\n",
    "\n",
    "# Step 2: Sort sub-basins by original glacier area (1980)\n",
    "glacier_stack = glacier_stack.sort_values('glacier_area_1980', ascending=False)\n",
    "\n",
    "# Step 3: Plot stacked horizontal bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Base: 2010 glacier area\n",
    "plt.barh(\n",
    "    glacier_stack['sub-basin'],\n",
    "    glacier_stack['area_remaining'],\n",
    "    label='Remaining Area (2010)',\n",
    "    color='#6baed6',\n",
    "    edgecolor='white'\n",
    ")\n",
    "\n",
    "# Overlay: Area lost since 1980\n",
    "plt.barh(\n",
    "    glacier_stack['sub-basin'],\n",
    "    glacier_stack['area_lost'],\n",
    "    left=glacier_stack['area_remaining'],\n",
    "    label='Area Lost (1980‚Äì2010)',\n",
    "    color='#fc9272',\n",
    "    edgecolor='white'\n",
    ")\n",
    "\n",
    "# Step 4: Styling\n",
    "plt.title('Glacier Area Lost vs Remaining by Sub-Basin (1980‚Äì2010)', fontsize=14, weight='bold')\n",
    "plt.xlabel('Glacier Area (km¬≤)', fontsize=12)\n",
    "plt.ylabel('Sub-Basin', fontsize=12)\n",
    "plt.gca().invert_yaxis()  # Largest area on top\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.legend(title='', frameon=False, fontsize=10)\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93784db2-4393-445d-9dc2-9efbe247511e",
   "metadata": {},
   "source": [
    "### ‚úÖ Extract District Centroids (Lat/Lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869c17b-10c3-4954-8841-64ce8798534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the district-level shapefile\n",
    "gdf = gpd.read_file(\"../data/local_unit_shapefiles/local_unit.shp\")\n",
    "\n",
    "# Step 2: Normalize district names\n",
    "gdf['DISTRICT'] = gdf['DISTRICT'].str.strip().str.upper()\n",
    "\n",
    "# Step 3: Dissolve to get one geometry per district\n",
    "district_gdf = gdf.dissolve(by='DISTRICT', as_index=False)\n",
    "\n",
    "# Step 4: Project to UTM Zone 45N (EPSG:32645) for accurate centroid calculation\n",
    "district_gdf_utm = district_gdf.to_crs(epsg=32645)\n",
    "\n",
    "# Step 5: Compute centroids in projected CRS\n",
    "district_gdf_utm['CENTROID'] = district_gdf_utm.geometry.centroid\n",
    "\n",
    "# Step 6: Reproject centroids to WGS84 (EPSG:4326) for lat/lon\n",
    "district_centroids_gdf = gpd.GeoDataFrame(\n",
    "    district_gdf_utm[['DISTRICT']],\n",
    "    geometry=district_gdf_utm['CENTROID'],\n",
    "    crs=\"EPSG:32645\"\n",
    ").to_crs(epsg=4326)\n",
    "\n",
    "# Step 7: Extract latitude and longitude\n",
    "district_gdf['CENTROID_LAT'] = district_centroids_gdf.geometry.y\n",
    "district_gdf['CENTROID_LON'] = district_centroids_gdf.geometry.x\n",
    "\n",
    "# Step 8: Final output: district name + lat/lon\n",
    "district_centroids = district_gdf[['DISTRICT', 'CENTROID_LAT', 'CENTROID_LON']].copy()\n",
    "\n",
    "# Step 9: Preview\n",
    "print(\"‚úÖ Spatial Centroids Extracted:\")\n",
    "print(district_centroids.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e362d1-d0a5-4b30-9b3c-c51406ee4e9f",
   "metadata": {},
   "source": [
    "### ‚úÖ Climate + Yield Features at District Centroids in Folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fabc97-0dee-48b6-98cf-77291c26364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- Step 1: Normalize district names to lowercase for merge ---\n",
    "district_centroids['DISTRICT'] = district_centroids['DISTRICT'].str.strip().str.lower()\n",
    "merged_df['district_name'] = merged_df['district_name'].str.strip().str.lower()\n",
    "\n",
    "# --- Step 2: Merge climate+yield features with centroids ---\n",
    "merged_map_df = pd.merge(\n",
    "    merged_df,\n",
    "    district_centroids,\n",
    "    left_on='district_name',\n",
    "    right_on='DISTRICT',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# --- Step 3: Filter for one specific year (e.g., 2010) ---\n",
    "map_data = merged_map_df[merged_map_df['year'] == 2010].copy()\n",
    "\n",
    "# --- Step 4: Create Folium map centered on Nepal ---\n",
    "m = folium.Map(location=[28.3, 84.0], zoom_start=7, tiles='CartoDB positron')\n",
    "\n",
    "# --- Step 5: Add markers with tooltips for each district ---\n",
    "for _, row in map_data.iterrows():\n",
    "    tooltip = (\n",
    "        f\"<b>{row['district_name'].title()}</b><br>\"\n",
    "        f\"Total Yield: {row['total_yield']:.1f} kg/ha<br>\"\n",
    "        f\"Avg Temp: {row['avg_temp']:.2f} ¬∞C<br>\"\n",
    "        f\"Precipitation: {row['annual_precip']:.1f} mm<br>\"\n",
    "        f\"Heatwave Days: {int(row['heatwave_days'])}\"\n",
    "    )\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "        location=[row['CENTROID_LAT'], row['CENTROID_LON']],\n",
    "        radius=6,\n",
    "        color='crimson',\n",
    "        fill=True,\n",
    "        fill_opacity=0.75,\n",
    "        tooltip=tooltip\n",
    "    ).add_to(m)\n",
    "\n",
    "# --- Step 6: Save and/or display the interactive map ---\n",
    "m.save(\"district_climate_yield_map.html\")\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f2b43-d8c0-4651-a7c6-5c9aa36f86a3",
   "metadata": {},
   "source": [
    "### ‚úÖ Merge Centroids with Merged Climate and Yield Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e5142-292a-4e3f-a6c3-4760ae1307c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Normalize district names for matching\n",
    "merged_df['district_name'] = merged_df['district_name'].str.strip().str.upper()\n",
    "district_centroids['DISTRICT'] = district_centroids['DISTRICT'].str.strip().str.upper()\n",
    "\n",
    "# Step 2: Merge spatial coordinates into the main DataFrame\n",
    "merged_with_coords = pd.merge(\n",
    "    merged_df,\n",
    "    district_centroids,\n",
    "    left_on='district_name',\n",
    "    right_on='DISTRICT',\n",
    "    how='left'\n",
    ").drop(columns=['DISTRICT'])  # Remove redundant key from right table\n",
    "\n",
    "# Step 3: Preview result\n",
    "print(f\"‚úÖ Merged Dataset with Spatial Coordinates: {merged_with_coords.shape[0]} rows √ó {merged_with_coords.shape[1]} columns\")\n",
    "print(merged_with_coords[['district_name', 'year', 'CENTROID_LAT', 'CENTROID_LON']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c653f3-a1f1-4705-88a9-3c6e4ef20e2a",
   "metadata": {},
   "source": [
    "### ‚úÖ MinMax and Standard Scaling (Merged, Unmerged features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e01bc34-5a7f-4e3e-9555-17151641beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# 1. Identify numeric feature columns (exclude ID, year, lat/lon)\n",
    "numeric_cols = merged_with_coords.select_dtypes(include='number').columns.tolist()\n",
    "exclude_cols = ['year', 'CENTROID_LAT', 'CENTROID_LON']\n",
    "features_to_scale = [col for col in numeric_cols if col not in exclude_cols]\n",
    "\n",
    "# 2. Apply StandardScaler (Z-score normalization)\n",
    "scaler_standard = StandardScaler()\n",
    "standard_scaled = pd.DataFrame(\n",
    "    scaler_standard.fit_transform(merged_with_coords[features_to_scale]),\n",
    "    columns=[f\"{col}_zscore\" for col in features_to_scale],\n",
    "    index=merged_with_coords.index\n",
    ")\n",
    "\n",
    "# 3. Apply MinMaxScaler (scale to 0‚Äì1)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "minmax_scaled = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(merged_with_coords[features_to_scale]),\n",
    "    columns=[f\"{col}_minmax\" for col in features_to_scale],\n",
    "    index=merged_with_coords.index\n",
    ")\n",
    "\n",
    "# 4. Concatenate scaled features with original dataframe\n",
    "merged_scaled = pd.concat([merged_with_coords, standard_scaled, minmax_scaled], axis=1)\n",
    "\n",
    "# 5. Preview result for first scaled feature\n",
    "first_feature = features_to_scale[0]\n",
    "print(f\"‚úÖ Scaled Dataset: {merged_scaled.shape[0]} rows √ó {merged_scaled.shape[1]} columns\")\n",
    "print(merged_scaled[[first_feature, f\"{first_feature}_zscore\", f\"{first_feature}_minmax\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733b3f20-cf0f-4868-88a2-0d29db47b399",
   "metadata": {},
   "source": [
    "### ‚úÖ Dimensionality Reduction (PCA) on Scaled Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcf3407-382b-4575-ae2a-22293598005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Select Z-score normalized columns\n",
    "zscore_cols = [col for col in merged_scaled.columns if col.endswith('_zscore')]\n",
    "\n",
    "# 2. Drop rows with missing values in selected columns\n",
    "X = merged_scaled[zscore_cols].dropna()\n",
    "\n",
    "# 3. Retain district and year metadata for reference\n",
    "meta = merged_scaled.loc[X.index, ['district_name', 'year']].reset_index(drop=True)\n",
    "\n",
    "# 4. Apply PCA to retain 95% of explained variance\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# 5. Create DataFrame of principal components\n",
    "pca_df = pd.DataFrame(X_pca, columns=[f'PC{i+1}' for i in range(X_pca.shape[1])])\n",
    "\n",
    "# 6. Combine PCA results with metadata\n",
    "pca_result = pd.concat([meta, pca_df], axis=1)\n",
    "\n",
    "# 7. Print summary and preview\n",
    "explained_var = pca.explained_variance_ratio_.sum()\n",
    "print(f\"‚úÖ PCA complete: {X_pca.shape[1]} components explain {explained_var:.2%} of variance.\")\n",
    "print(pca_result.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a0f8df-b559-4130-a525-9901c64b7d75",
   "metadata": {},
   "source": [
    "### ‚úÖ Top Contributing Features for Each Principal Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d5145-10b4-4f26-a268-429de3995ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assume `pca` is already fitted and `X.columns` contains the original feature names\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    index=X.columns,\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)]\n",
    ")\n",
    "\n",
    "# Extract top 5 contributors for each principal component\n",
    "top_features = {}\n",
    "\n",
    "print(\"üîç Top Contributing Features per Principal Component:\")\n",
    "for pc in loadings.columns:\n",
    "    # Sort by absolute loading strength\n",
    "    sorted_loadings = loadings[pc].abs().sort_values(ascending=False)\n",
    "    top_feats = sorted_loadings.head(5).index.tolist()\n",
    "    top_features[pc] = top_feats\n",
    "\n",
    "    # Print formatted output\n",
    "    print(f\"\\nüìå {pc} ‚Äî Top 5 Features:\")\n",
    "    for feat in top_feats:\n",
    "        raw_weight = loadings.loc[feat, pc]\n",
    "        print(f\"  {feat:<30} ‚Üí loading: {raw_weight:+.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ca0f5-de39-4bee-bf81-8a09afff24e5",
   "metadata": {},
   "source": [
    "### ‚úÖ Feature Sets for ML Model Training (Original, Scaled, PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c4537c-d2ff-44a0-a7c6-88c2b041c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Original numeric features (excluding index-like columns) ---\n",
    "original_features = merged_with_coords.select_dtypes(include='number').columns.tolist()\n",
    "print(\"üîπ Original Numeric Features:\")\n",
    "print(original_features)\n",
    "\n",
    "# --- 2. Scaled features ---\n",
    "zscore_features = [col for col in merged_scaled.columns if col.endswith('_zscore')]\n",
    "minmax_features = [col for col in merged_scaled.columns if col.endswith('_minmax')]\n",
    "\n",
    "print(\"\\nüîπ Z-Score Scaled Features:\")\n",
    "print(zscore_features)\n",
    "\n",
    "print(\"\\nüîπ MinMax Scaled Features:\")\n",
    "print(minmax_features)\n",
    "\n",
    "# --- 3. PCA-reduced features (excluding any metadata columns if present) ---\n",
    "pca_features = [col for col in pca_df.columns if col.startswith('PC')]\n",
    "\n",
    "print(\"\\nüîπ PCA-Reduced Features:\")\n",
    "print(pca_features)\n",
    "\n",
    "# --- 4. Summary ---\n",
    "print(\"\\n‚úÖ Feature Summary:\")\n",
    "print(f\"‚Ä¢ Original Numeric Features: {len(original_features)}\")\n",
    "print(f\"‚Ä¢ Z-Score Scaled Features:   {len(zscore_features)}\")\n",
    "print(f\"‚Ä¢ MinMax Scaled Features:    {len(minmax_features)}\")\n",
    "print(f\"‚Ä¢ PCA Components:            {len(pca_features)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c193e84f-21c9-42fa-9cdb-6fb285ba9d9e",
   "metadata": {},
   "source": [
    "### ‚úÖ Save All Required DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c88430e-0a16-485a-a266-c1a1b0fb20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# Save all necessary data\n",
    "climate_yearly.to_csv(\"../data/processed/climate_yearly.csv\", index=False)\n",
    "merged_with_coords.to_csv(\"../data/processed/merged_with_coords.csv\", index=False)\n",
    "merged_scaled.to_csv(\"../data/processed/merged_scaled.csv\", index=False)\n",
    "glacier_features.to_csv(\"../data/processed/glacier_features.csv\", index=False)\n",
    "glacier_long.to_csv(\"../data/processed/glacier_long.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ All key feature datasets saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77ca492-dde1-4026-b6bc-4845b5a95f3b",
   "metadata": {},
   "source": [
    "## ‚úÖ 6. Machine Learning Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc458886-7d52-47ea-8d9f-4bf2000e9389",
   "metadata": {},
   "source": [
    "### ‚úÖ Load the Required Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52b8ed-8196-4b74-a79c-7f576cbb38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load preprocessed feature datasets\n",
    "climate_yearly = pd.read_csv(\"../data/processed/climate_yearly.csv\")\n",
    "merged_with_coords = pd.read_csv(\"../data/processed/merged_with_coords.csv\")\n",
    "merged_scaled = pd.read_csv(\"../data/processed/merged_scaled.csv\")\n",
    "glacier_features = pd.read_csv(\"../data/processed/glacier_features.csv\")\n",
    "glacier_long = pd.read_csv(\"../data/processed/glacier_long.csv\")\n",
    "\n",
    "print(\"‚úÖ All datasets loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cf16ce-b78d-4455-b1cd-e1309dabea1c",
   "metadata": {},
   "source": [
    "### üîπ Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bd510-8ce0-4722-a2e8-b37cad854756",
   "metadata": {},
   "source": [
    "### ‚úÖ Climate Zone Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd781d32-00ed-4429-992f-b40e1deb2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# --- Step 0: Load data and ensure output directory ---\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "try:\n",
    "    merged_with_coords\n",
    "except NameError:\n",
    "    merged_with_coords = pd.read_csv(\"../data/processed/merged_with_coords.csv\")\n",
    "    print(\"‚úÖ Loaded merged_with_coords.\")\n",
    "\n",
    "# --- Step 1: Assign climate zones if not present ---\n",
    "def assign_climate_zone(row):\n",
    "    if row['avg_temp'] >= 25:\n",
    "        return 'Tropical'\n",
    "    elif row['avg_temp'] >= 15:\n",
    "        return 'Subtropical'\n",
    "    elif row['avg_temp'] >= 5:\n",
    "        return 'Temperate'\n",
    "    else:\n",
    "        return 'Alpine'\n",
    "\n",
    "if 'climate_zone' not in merged_with_coords.columns:\n",
    "    merged_with_coords['climate_zone'] = merged_with_coords.apply(assign_climate_zone, axis=1)\n",
    "\n",
    "# --- Step 2: Define features and target ---\n",
    "features = [\n",
    "    'avg_temp', 'avg_max_temp', 'annual_precip',\n",
    "    'avg_humidity', 'temp_range_stddev', 'heatwave_days'\n",
    "]\n",
    "target = 'climate_zone'\n",
    "\n",
    "# Drop rows with missing values\n",
    "data = merged_with_coords.dropna(subset=features + [target]).copy()\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "\n",
    "# --- Step 3: Split data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# --- Step 4: Define models ---\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "summary = []\n",
    "\n",
    "# --- Step 5: Train, Evaluate, Save ---\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç {name} Evaluation:\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"‚úÖ Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    print(\"üìâ Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Cross-validation\n",
    "    cv = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    summary.append({\n",
    "        'Model': name,\n",
    "        'Test Accuracy': model.score(X_test, y_test),\n",
    "        'CV Accuracy': np.mean(cv),\n",
    "        'CV Std': np.std(cv)\n",
    "    })\n",
    "\n",
    "    # Save trained model\n",
    "    model_key = name.lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    filename = f\"../data/processed/climate_zone_{model_key}.joblib\"\n",
    "    joblib.dump(model, filename)\n",
    "    print(f\"üíæ Saved to: {filename}\")\n",
    "\n",
    "    # Show feature importances if available\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"üìå Top Feature Importances:\")\n",
    "        importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        for feat, score in importances.items():\n",
    "            print(f\"  {feat:<30} ‚Üí {score:.4f}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Feature importances not available.\")\n",
    "\n",
    "# --- Step 6: Summary ---\n",
    "print(\"\\nüìã Model Summary:\")\n",
    "print(pd.DataFrame(summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d039d3-837d-4531-afe7-64f0a42c9d49",
   "metadata": {},
   "source": [
    "### ‚úÖ Extreme Heatwave Classification based on district-year climate conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e2864-e0fe-4ae5-a454-145dfd302681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# === Step 1: Load data ===\n",
    "# Uncomment if needed\n",
    "# climate_yearly = pd.read_csv(\"../data/processed/climate_yearly.csv\")\n",
    "\n",
    "# === Step 2: Define binary heatwave label ===\n",
    "threshold = 30\n",
    "climate_yearly['heatwave_year'] = (climate_yearly['heatwave_days'] >= threshold).astype(int)\n",
    "\n",
    "# === Step 3: Prepare features and labels ===\n",
    "X = climate_yearly.drop(columns=[\n",
    "    'DISTRICT', 'YEAR', 'heatwave_days', 'heatwave_year'\n",
    "])\n",
    "X = X.select_dtypes(include=[np.number]).dropna()\n",
    "y = climate_yearly.loc[X.index, 'heatwave_year']\n",
    "\n",
    "# === Step 4: Train/Test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 5: Initialize models ===\n",
    "models = {\n",
    "    'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'svm_rbf': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# === Step 6: Create output folder ===\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "summary = []\n",
    "\n",
    "# === Step 7: Train, Evaluate, Save ===\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç {name.upper()} Evaluation\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"‚úÖ Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    print(\"üìâ Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Cross-validation\n",
    "    cv = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    summary.append({\n",
    "        'Model': name,\n",
    "        'Test Accuracy': model.score(X_test, y_test),\n",
    "        'CV Accuracy': np.mean(cv),\n",
    "        'CV Std': np.std(cv)\n",
    "    })\n",
    "\n",
    "    # Feature importances (for tree-based models)\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        print(\"üìå Feature Importances:\")\n",
    "        importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        print(importances.head(10))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Feature importances not available for this model.\")\n",
    "\n",
    "    # Save model\n",
    "    path = f\"../data/processed/heatwave_model_{name}.joblib\"\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"üíæ Model saved to: {path}\")\n",
    "\n",
    "# === Step 8: Summary ===\n",
    "print(\"\\nüìã Model Performance Summary:\")\n",
    "print(pd.DataFrame(summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a94264-5953-44e3-9409-b32656bb555a",
   "metadata": {},
   "source": [
    "### ‚úÖ Drought Risk Category Classification using existing SPI proxy (precip_zscore) in climate_yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051617f-9256-42d6-a78f-d68e69a5666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- Step 0: Load dataset if not already loaded ---\n",
    "# climate_yearly = pd.read_csv(\"../data/processed/climate_yearly.csv\")\n",
    "\n",
    "# --- Step 1: Ensure output directory exists ---\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# --- Step 2: Classify drought risk based on SPI-like z-score ---\n",
    "def classify_spi(z):\n",
    "    if z >= -0.5:\n",
    "        return \"None\"\n",
    "    elif z >= -1.0:\n",
    "        return \"Mild\"\n",
    "    elif z >= -1.5:\n",
    "        return \"Moderate\"\n",
    "    elif z >= -2.0:\n",
    "        return \"Severe\"\n",
    "    else:\n",
    "        return \"Extreme\"\n",
    "\n",
    "climate_yearly['drought_risk'] = climate_yearly['precip_zscore'].apply(classify_spi)\n",
    "\n",
    "# --- Step 3: Define features and target ---\n",
    "X = climate_yearly.drop(columns=[\n",
    "    'heatwave_days', 'heatwave_year', 'drought_risk',\n",
    "    'DISTRICT', 'YEAR'\n",
    "])\n",
    "X = X.select_dtypes(include=[np.number]).dropna()\n",
    "y = climate_yearly.loc[X.index, 'drought_risk']\n",
    "\n",
    "# --- Step 4: Encode target labels ---\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# --- Step 5: Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.25, stratify=y_encoded, random_state=42\n",
    ")\n",
    "\n",
    "# --- Step 6: Define classifiers ---\n",
    "models = {\n",
    "    'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'svm_rbf': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "summary = []\n",
    "\n",
    "# --- Step 7: Train, evaluate, and save ---\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç {name.upper()} Evaluation\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"‚úÖ Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))\n",
    "\n",
    "    print(\"üìâ Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X, y_encoded, cv=5, scoring='accuracy')\n",
    "    summary.append({\n",
    "        'Model': name,\n",
    "        'Test Accuracy': model.score(X_test, y_test),\n",
    "        'CV Accuracy': np.mean(cv_scores),\n",
    "        'CV Std': np.std(cv_scores)\n",
    "    })\n",
    "\n",
    "    # Feature importances\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"üìå Feature Importances:\")\n",
    "        importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        print(importances.head(10))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Feature importances not available for this model.\")\n",
    "\n",
    "    # Save model\n",
    "    path = f\"../data/processed/drought_model_{name}.joblib\"\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"üíæ Model saved to: {path}\")\n",
    "\n",
    "# --- Step 8: Print Summary ---\n",
    "print(\"\\nüìã Model Performance Summary:\")\n",
    "print(pd.DataFrame(summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a643a-dee5-41fd-832f-106009877ae4",
   "metadata": {},
   "source": [
    "### ‚úÖ Cereal Yield Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4af223-a63d-4ab9-9065-5f69a9eb9823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# === Step 0: Load dataset if not already loaded ===\n",
    "# merged_scaled = pd.read_csv(\"../data/processed/merged_scaled.csv\")\n",
    "\n",
    "# === Step 1: Ensure output directory exists ===\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# === Step 2: Create Binary Yield Label ===\n",
    "threshold = merged_scaled['total_yield'].median()\n",
    "merged_scaled['yield_class'] = (merged_scaled['total_yield'] > threshold).astype(int)\n",
    "\n",
    "# === Step 3: Define Features and Labels ===\n",
    "X_raw = merged_scaled.drop(columns=[\n",
    "    'total_yield', 'yield_class', 'district_name', 'year',\n",
    "    'CENTROID_LAT', 'CENTROID_LON'\n",
    "])\n",
    "\n",
    "X = pd.get_dummies(X_raw, drop_first=True)\n",
    "y = merged_scaled['yield_class']\n",
    "\n",
    "# === Step 4: Drop rows with missing values ===\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# === Step 5: Train/Test Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 6: Define and Train Models ===\n",
    "models = {\n",
    "    'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'svm_rbf': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "summary = []\n",
    "\n",
    "# === Step 7: Train, Evaluate, Save ===\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç {name.upper()} Evaluation:\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"‚úÖ Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    print(\"üìâ Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    summary.append({\n",
    "        'Model': name,\n",
    "        'Test Accuracy': model.score(X_test, y_test),\n",
    "        'CV Accuracy': np.mean(cv_scores),\n",
    "        'CV Std': np.std(cv_scores)\n",
    "    })\n",
    "\n",
    "    # Save model\n",
    "    model_path = f\"../data/processed/yield_model_{name}.joblib\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"üíæ Model saved to: {model_path}\")\n",
    "\n",
    "    # Feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"üìå Top Feature Importances:\")\n",
    "        importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        print(importances.head(10))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Feature importances not available for this model.\")\n",
    "\n",
    "# === Step 8: Summary Table ===\n",
    "print(\"\\nüìã Model Summary:\")\n",
    "print(pd.DataFrame(summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb1b938-20c7-4ec3-a87e-69c1601a195b",
   "metadata": {},
   "source": [
    "#### ‚úÖ Glacier Retreat Severity Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db73b3-da7b-48cc-abe8-2a385f7c8680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# --- Step 0: Ensure output directory exists ---\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# --- Step 1: Features and target ---\n",
    "# glacier_features = pd.read_csv(\"../data/processed/glacier_features.csv\")  # Uncomment if needed\n",
    "X = glacier_features[[\n",
    "    'glacier_area_1980', 'glacier_area_2010',\n",
    "    'ice_volume_1980', 'ice_volume_2010',\n",
    "    'min_elev_1980', 'min_elev_2010',\n",
    "    'area_loss_km2', 'area_loss_pct',\n",
    "    'volume_loss_km3', 'volume_loss_pct',\n",
    "    'elev_rise_m'\n",
    "]]\n",
    "y = glacier_features['retreat_severity']\n",
    "\n",
    "# --- Step 2: Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- Step 3: Models ---\n",
    "models = {\n",
    "    'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'svm_rbf': SVC(kernel='rbf', probability=True, random_state=42),\n",
    "    'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "summary = []\n",
    "\n",
    "# --- Step 4: Train, evaluate, and save models ---\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç {name.upper()} Evaluation:\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Report\n",
    "    print(\"‚úÖ Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    \n",
    "    print(\"üìâ Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # CV Score\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    cv_mean, cv_std = scores.mean(), scores.std()\n",
    "    print(f\"üìä 5-Fold CV Accuracy: {cv_mean:.3f} ¬± {cv_std:.3f}\")\n",
    "\n",
    "    # Feature importances\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        print(\"üìå Feature Importances:\")\n",
    "        importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        for feat, val in importances.items():\n",
    "            print(f\"  {feat:<25} ‚Üí {val:.4f}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Feature importance not available for this model.\")\n",
    "\n",
    "    # Save model\n",
    "    model_path = f\"../data/processed/glacier_model_{name}.joblib\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"üíæ Model saved to: {model_path}\")\n",
    "\n",
    "    # Add to summary\n",
    "    summary.append({\n",
    "        'Model': name,\n",
    "        'Test Accuracy': model.score(X_test, y_test),\n",
    "        'CV Accuracy Mean': cv_mean,\n",
    "        'CV Accuracy Std': cv_std\n",
    "    })\n",
    "\n",
    "# --- Step 5: Summary Table ---\n",
    "print(\"\\nüìã Summary Comparison:\")\n",
    "print(pd.DataFrame(summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0367a-440f-40a2-9dc6-a67cdc006526",
   "metadata": {},
   "source": [
    "### üîπ Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bef07a-ee7e-407c-8fe4-6cafeee5bb39",
   "metadata": {},
   "source": [
    "### ‚úÖ Cereal Yield Prediction (Regression Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767314fc-3578-445a-9477-21e83650cac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# --- Step 0: Ensure output directory exists ---\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# --- Step 1: Define target and features ---\n",
    "# y = merged_scaled['total_yield']  # Uncomment if not loaded\n",
    "# X_raw = merged_scaled.drop(columns=[ ... ])  # Uncomment if not defined\n",
    "\n",
    "y = merged_scaled['total_yield']\n",
    "X_raw = merged_scaled.drop(columns=[\n",
    "    'total_yield', 'yield_class', 'district_name', 'year',\n",
    "    'CENTROID_LAT', 'CENTROID_LON'\n",
    "])\n",
    "X = pd.get_dummies(X_raw, drop_first=True)\n",
    "\n",
    "# --- Step 2: Drop missing values ---\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# --- Step 3: Train-test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# --- Step 4: Define regression models ---\n",
    "models = {\n",
    "    'linear_regression': LinearRegression(),\n",
    "    'ridge_regression': Ridge(alpha=1.0),\n",
    "    'lasso_regression': Lasso(alpha=0.1),\n",
    "    'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "summary = []\n",
    "\n",
    "# --- Step 5: Train, Evaluate, Save ---\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç {name.replace('_', ' ').title()} Evaluation:\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"üìà RMSE: {rmse:.2f}\")\n",
    "    print(f\"üìâ MAE : {mae:.2f}\")\n",
    "    print(f\"üîÅ R¬≤  : {r2:.4f}\")\n",
    "\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "    summary.append({\n",
    "        'Model': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤ Score': r2,\n",
    "        'CV R¬≤ Mean': np.mean(cv_scores),\n",
    "        'CV R¬≤ Std': np.std(cv_scores)\n",
    "    })\n",
    "\n",
    "    # Save model to disk\n",
    "    model_path = f\"../data/processed/yield_regressor_{name}.joblib\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"üíæ Model saved to: {model_path}\")\n",
    "\n",
    "    # Display feature importance or coefficients\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"üìå Top Feature Importances:\")\n",
    "        importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        print(importances.head(10))\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        print(\"üìå Top Coefficients:\")\n",
    "        coefs = pd.Series(model.coef_, index=X.columns).sort_values(key=np.abs, ascending=False)\n",
    "        print(coefs.head(10))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Feature importance not available.\")\n",
    "\n",
    "# --- Step 6: Print summary ---\n",
    "print(\"\\nüìã Regression Model Summary:\")\n",
    "print(pd.DataFrame(summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8732091-1fdf-4e54-bc66-2b5b84e9d86e",
   "metadata": {},
   "source": [
    "### ‚úÖ Glacier Area and Volume Loss Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b4810-b0a0-4c8e-ae68-8536ef560746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- Step 0: Load dataset (uncomment if needed) ---\n",
    "# glacier_features = pd.read_csv(\"../data/processed/glacier_features.csv\")\n",
    "\n",
    "# --- 1. Define targets ---\n",
    "y_area = glacier_features['area_loss_km2']\n",
    "y_volume = glacier_features['volume_loss_km3']\n",
    "\n",
    "# --- 2. Define feature set ---\n",
    "X = glacier_features.drop(columns=[\n",
    "    'area_loss_km2', 'volume_loss_km3', 'retreat_severity',\n",
    "    'area_loss_pct', 'volume_loss_pct',\n",
    "    'basin', 'sub-basin'  # IDs\n",
    "])\n",
    "\n",
    "# --- 3. Drop missing values ---\n",
    "X = X.dropna()\n",
    "y_area = y_area.loc[X.index]\n",
    "y_volume = y_volume.loc[X.index]\n",
    "\n",
    "# --- 4. Train/test split ---\n",
    "X_train, X_test, ya_train, ya_test = train_test_split(X, y_area, test_size=0.25, random_state=42)\n",
    "_, _, yv_train, yv_test = train_test_split(X, y_volume, test_size=0.25, random_state=42)\n",
    "\n",
    "# --- 5. Define models ---\n",
    "models = {\n",
    "    'linear_regression': LinearRegression(),\n",
    "    'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# --- 6. Evaluation and saving ---\n",
    "def evaluate_and_save_model(name, model, X_train, y_train, X_test, y_test, X_all, y_all, label):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\nüîç {name.upper()} ({label}) Regression:\")\n",
    "    print(f\"üìà RMSE: {rmse:.2f} | üìâ MAE: {mae:.2f} | üîÅ R¬≤: {r2:.4f}\")\n",
    "\n",
    "    cv_scores = cross_val_score(model, X_all, y_all, cv=5, scoring='r2')\n",
    "    print(f\"üìä CV R¬≤: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"üìå Top Feature Importances:\")\n",
    "        fi = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        print(fi.head(5))\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        print(\"üìå Top Coefficients:\")\n",
    "        coefs = pd.Series(model.coef_, index=X.columns).sort_values(key=np.abs, ascending=False)\n",
    "        print(coefs.head(5))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Feature importances not available.\")\n",
    "\n",
    "    path = f\"../data/processed/glacier_regressor_{label}_{name}.joblib\"\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"üíæ Model saved to: {path}\")\n",
    "\n",
    "# --- 7. Predict and save for Area Loss ---\n",
    "print(\"\\nüåê Predicting Glacier Area Loss:\")\n",
    "for name, model in models.items():\n",
    "    evaluate_and_save_model(name, model, X_train, ya_train, X_test, ya_test, X, y_area, \"area\")\n",
    "\n",
    "# --- 8. Predict and save for Volume Loss ---\n",
    "print(\"\\n‚ùÑÔ∏è Predicting Glacier Volume Loss:\")\n",
    "for name, model in models.items():\n",
    "    evaluate_and_save_model(name, model, X_train, yv_train, X_test, yv_test, X, y_volume, \"volume\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d150059-9d75-497a-9446-aed52c43d12b",
   "metadata": {},
   "source": [
    "### ‚úÖ Heatwave Days Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11006297-dff3-47ac-95b9-ccf6fe0f410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# === Step 0: Ensure output directory exists ===\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# === Step 1: Define target ===\n",
    "# climate_yearly = pd.read_csv(\"../data/processed/climate_yearly.csv\")  # Uncomment if not loaded\n",
    "y = climate_yearly['heatwave_days']\n",
    "\n",
    "# === Step 2: Define features ===\n",
    "X = climate_yearly.drop(columns=[\n",
    "    'DISTRICT', 'YEAR', 'heatwave_days', 'heatwave_year'  # remove ID/leakage\n",
    "])\n",
    "\n",
    "# One-hot encode if any categorical columns exist\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Drop rows with missing data\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# === Step 3: Split data ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# === Step 4: Define models ===\n",
    "models = {\n",
    "    'linear_regression': LinearRegression(),\n",
    "    'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# === Step 5: Train and evaluate ===\n",
    "summary = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç {name.replace('_', ' ').title()} Regression:\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"üìà RMSE: {rmse:.2f}\")\n",
    "    print(f\"üìâ MAE : {mae:.2f}\")\n",
    "    print(f\"üîÅ R¬≤  : {r2:.4f}\")\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "    print(f\"üìä CV R¬≤: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "\n",
    "    # Feature insights\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"üìå Top Feature Importances:\")\n",
    "        fi = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        print(fi.head(10))\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        print(\"üìå Top Coefficients:\")\n",
    "        coefs = pd.Series(model.coef_, index=X.columns).sort_values(key=np.abs, ascending=False)\n",
    "        print(coefs.head(10))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Feature importances not available.\")\n",
    "\n",
    "    # Save model\n",
    "    model_path = f\"../data/processed/heatwave_regressor_{name}.joblib\"\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"üíæ Model saved to: {model_path}\")\n",
    "\n",
    "    summary.append({\n",
    "        'Model': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤ Score': r2,\n",
    "        'CV R¬≤ Mean': cv_scores.mean(),\n",
    "        'CV R¬≤ Std': cv_scores.std()\n",
    "    })\n",
    "\n",
    "# === Step 6: Summary Table ===\n",
    "print(\"\\nüìã Regression Model Summary:\")\n",
    "print(pd.DataFrame(summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a04bb-04f3-40f5-96c9-4129f7cf8b4c",
   "metadata": {},
   "source": [
    "### ‚úÖ Drought Severity Regression using precip_zscore (SPI proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac79cc1-4ba9-44d0-80f9-f004d56b3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# === Ensure output directory exists ===\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# === 1. Define target and features ===\n",
    "y = climate_yearly['precip_zscore']  # SPI-like drought index\n",
    "\n",
    "X = climate_yearly.drop(columns=[\n",
    "    'precip_zscore', 'drought_risk', 'heatwave_days', 'heatwave_year',\n",
    "    'DISTRICT', 'YEAR'\n",
    "])\n",
    "\n",
    "# Convert categoricals (if any)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Drop rows with missing values\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "# === 2. Train/test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# === 3. Define regression models ===\n",
    "models = {\n",
    "    'linear_regression': LinearRegression(),\n",
    "    'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "summary = []\n",
    "\n",
    "# === 4. Evaluate and save each model ===\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüîç {name.upper()} Regression:\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"üìà RMSE: {rmse:.4f}\")\n",
    "    print(f\"üìâ MAE : {mae:.4f}\")\n",
    "    print(f\"üîÅ R¬≤  : {r2:.4f}\")\n",
    "\n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "    print(f\"üìä CV R¬≤: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "\n",
    "    # Save summary\n",
    "    summary.append({\n",
    "        'Model': name,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤ Score': r2,\n",
    "        'CV R¬≤ Mean': cv_scores.mean(),\n",
    "        'CV R¬≤ Std': cv_scores.std()\n",
    "    })\n",
    "\n",
    "    # Feature importances / coefficients\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        print(\"üìå Top Feature Importances:\")\n",
    "        importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        print(importances.head(10))\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        print(\"üìå Top Coefficients:\")\n",
    "        coefs = pd.Series(model.coef_, index=X.columns).sort_values(key=np.abs, ascending=False)\n",
    "        print(coefs.head(10))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Feature importance not available.\")\n",
    "\n",
    "    # Save model\n",
    "    path = f\"../data/processed/drought_regressor_{name}.joblib\"\n",
    "    joblib.dump(model, path)\n",
    "    print(f\"üíæ Model saved to: {path}\")\n",
    "\n",
    "# === 5. Print Summary Table ===\n",
    "print(\"\\nüìã Regression Model Summary:\")\n",
    "print(pd.DataFrame(summary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b384eaf6-44a4-451e-9c0f-dad3bdd1b3a2",
   "metadata": {},
   "source": [
    "### üîπ Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661923fc-06b7-47b9-9687-ee7137672644",
   "metadata": {},
   "source": [
    "### ‚úÖ Heatwarve Days Forecasting up to 2050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb87656-33f3-4cee-b4bd-9a4b494c6e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import os\n",
    "\n",
    "# === 1. Define input features ===\n",
    "features = [\n",
    "    'avg_temp', 'avg_max_temp', 'temp_range_stddev', 'avg_humidity',\n",
    "    'avg_wind', 'annual_precip', 'precip_zscore',\n",
    "    'avg_temp_lag1', 'annual_precip_lag1', 'precip_zscore_lag1',\n",
    "    'temp_range_stddev_lag1', 'heatwave_days_lag1'\n",
    "]\n",
    "\n",
    "# === 2. Load and prepare historical data ===\n",
    "# climate_yearly = pd.read_csv(\"../data/processed/climate_yearly.csv\")  # Uncomment if needed\n",
    "df = climate_yearly.copy()\n",
    "df_model = df[['DISTRICT', 'YEAR', 'heatwave_days'] + features].dropna()\n",
    "\n",
    "X = df_model[features]\n",
    "y = df_model['heatwave_days']\n",
    "\n",
    "# === 3. Train model ===\n",
    "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# === 4. Prepare forecast years and districts ===\n",
    "future_years = list(range(2020, 2051))\n",
    "districts = df_model['DISTRICT'].unique()\n",
    "forecast_rows = []\n",
    "\n",
    "# === 5. Simulate future values per district ===\n",
    "for district in districts:\n",
    "    district_df = df_model[df_model['DISTRICT'] == district]\n",
    "    if district_df.empty:\n",
    "        continue\n",
    "\n",
    "    last_row = district_df.loc[district_df['YEAR'].idxmax()].copy()\n",
    "\n",
    "    for year in future_years:\n",
    "        new_row = {'DISTRICT': district, 'YEAR': year}\n",
    "\n",
    "        for col in features:\n",
    "            if 'lag1' in col:\n",
    "                base_col = col.replace('_lag1', '')\n",
    "                val = last_row.get(base_col, df_model[base_col].mean())\n",
    "                new_row[col] = val\n",
    "            else:\n",
    "                val = last_row.get(col, df_model[col].mean())\n",
    "                new_row[col] = val + np.random.normal(0, 0.1)  # small noise\n",
    "\n",
    "        forecast_rows.append(new_row)\n",
    "        last_row = pd.Series(new_row)\n",
    "\n",
    "# === 6. Predict and output ===\n",
    "forecast_df = pd.DataFrame(forecast_rows)\n",
    "forecast_df = forecast_df.dropna(subset=features)\n",
    "forecast_df['predicted_heatwave_days'] = model.predict(forecast_df[features])\n",
    "\n",
    "# === 7. Save output ===\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "forecast_df.to_csv(\"../data/processed/heatwave_days_forecast_2020_2050.csv\", index=False)\n",
    "\n",
    "# === 8. Preview ===\n",
    "print(\"‚úÖ Forecast Preview (2050):\")\n",
    "print(forecast_df[forecast_df['YEAR'] == 2050][['DISTRICT', 'YEAR', 'predicted_heatwave_days']].round(1))\n",
    "print(\"üíæ Forecast saved to: ../data/processed/heatwave_days_forecast_2020_2050.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a9c4b3-0fd5-4407-9f62-6989790acb66",
   "metadata": {},
   "source": [
    "### ‚úÖ Forecasting Drought Severity (SPI Proxy) to 2050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f54740d-3f3b-41ec-b73f-093cc76ee424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import os\n",
    "\n",
    "# --- 1. Define features used for SPI prediction ---\n",
    "features = [\n",
    "    'annual_precip', 'avg_temp', 'avg_max_temp', 'avg_humidity',\n",
    "    'avg_wind', 'temp_range_stddev',\n",
    "    'annual_precip_lag1', 'avg_temp_lag1', 'temp_range_stddev_lag1'\n",
    "]\n",
    "target = 'precip_zscore'  # SPI proxy\n",
    "\n",
    "# --- 2. Load and filter dataset ---\n",
    "# climate_yearly = pd.read_csv(\"../data/processed/climate_yearly.csv\")  # Uncomment if needed\n",
    "df = climate_yearly.copy()\n",
    "df_model = df[['DISTRICT', 'YEAR', target] + features].dropna()\n",
    "\n",
    "# --- 3. Train Gradient Boosting model on historical SPI data ---\n",
    "X = df_model[features]\n",
    "y = df_model[target]\n",
    "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# --- 4. Forecast SPI for each district from 2020 to 2050 ---\n",
    "future_years = list(range(2020, 2051))\n",
    "districts = df_model['DISTRICT'].unique()\n",
    "forecast_rows = []\n",
    "\n",
    "for district in districts:\n",
    "    district_df = df_model[df_model['DISTRICT'] == district].copy()\n",
    "    if district_df.empty:\n",
    "        continue\n",
    "\n",
    "    last_known = district_df[district_df['YEAR'] == district_df['YEAR'].max()]\n",
    "    if last_known.empty:\n",
    "        continue\n",
    "\n",
    "    last_row = last_known.iloc[0].copy()\n",
    "\n",
    "    for year in future_years:\n",
    "        new_row = {'DISTRICT': district, 'YEAR': year}\n",
    "        for col in features:\n",
    "            if 'lag1' in col:\n",
    "                base_col = col.replace('_lag1', '')\n",
    "                val = last_row.get(base_col, df_model[base_col].mean())\n",
    "                new_row[col] = val\n",
    "            else:\n",
    "                base_val = last_row.get(col, df_model[col].mean())\n",
    "                new_row[col] = base_val + np.random.normal(0, 0.1)\n",
    "        forecast_rows.append(new_row)\n",
    "        last_row = pd.Series(new_row)\n",
    "\n",
    "# --- 5. Predict SPI and classify drought severity ---\n",
    "forecast_df = pd.DataFrame(forecast_rows)\n",
    "forecast_df = forecast_df.dropna(subset=features)\n",
    "forecast_df['predicted_spi'] = model.predict(forecast_df[features])\n",
    "\n",
    "def classify_spi(z):\n",
    "    if z >= -0.5:\n",
    "        return \"None\"\n",
    "    elif z >= -1.0:\n",
    "        return \"Mild\"\n",
    "    elif z >= -1.5:\n",
    "        return \"Moderate\"\n",
    "    elif z >= -2.0:\n",
    "        return \"Severe\"\n",
    "    else:\n",
    "        return \"Extreme\"\n",
    "\n",
    "forecast_df['drought_risk'] = forecast_df['predicted_spi'].apply(classify_spi)\n",
    "\n",
    "# --- 6. Save and preview forecast ---\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "forecast_path = \"../data/processed/drought_forecast_spi_2020_2050.csv\"\n",
    "forecast_df.to_csv(forecast_path, index=False)\n",
    "\n",
    "print(\"‚úÖ Drought Forecast Preview (2050):\")\n",
    "print(forecast_df[forecast_df['YEAR'] == 2050][['DISTRICT', 'YEAR', 'predicted_spi', 'drought_risk']].round(2))\n",
    "print(f\"üíæ Forecast saved to: {forecast_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d2f2ea-05b8-499c-93aa-29bee7b5439b",
   "metadata": {},
   "source": [
    "### ‚úÖ Climate Forecast Up to 2050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0b3dc-289f-4910-a2e4-a358d8a18981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import joblib\n",
    "\n",
    "# === 1. Define features for forecasting ===\n",
    "features = [\n",
    "    'avg_temp', 'avg_max_temp', 'temp_range_stddev', 'avg_humidity',\n",
    "    'avg_wind', 'annual_precip',\n",
    "    'avg_temp_lag1', 'annual_precip_lag1', 'temp_range_stddev_lag1'\n",
    "]\n",
    "\n",
    "# === 2. Load historical dataset ===\n",
    "# climate_yearly = pd.read_csv(\"../data/processed/climate_yearly.csv\")  # Uncomment if not loaded\n",
    "df = climate_yearly.copy()\n",
    "df_model = df[['DISTRICT', 'YEAR'] + features].dropna()\n",
    "\n",
    "# === 3. Train Gradient Boosting model on historical avg_temp ===\n",
    "X = df_model[features]\n",
    "y = df_model['avg_temp']\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save trained model\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "joblib.dump(model, \"../data/processed/climate_regressor_avg_temp_gradient_boosting.joblib\")\n",
    "\n",
    "# === 4. Define forecast range ===\n",
    "future_years = list(range(2020, 2051))\n",
    "districts = df_model['DISTRICT'].unique()\n",
    "forecast_rows = []\n",
    "\n",
    "# === 5. Forward simulate features by district ===\n",
    "for district in districts:\n",
    "    district_df = df_model[df_model['DISTRICT'] == district]\n",
    "    if district_df.empty:\n",
    "        continue\n",
    "\n",
    "    last_row = district_df.loc[district_df['YEAR'].idxmax()].copy()\n",
    "\n",
    "    for year in future_years:\n",
    "        new_row = {'DISTRICT': district, 'YEAR': year}\n",
    "\n",
    "        for col in features:\n",
    "            if 'lag1' in col:\n",
    "                base_col = col.replace('_lag1', '')\n",
    "                val = last_row.get(base_col, df_model[base_col].mean())\n",
    "                new_row[col] = val\n",
    "            else:\n",
    "                val = last_row.get(col, df_model[col].mean())\n",
    "                new_row[col] = val + np.random.normal(0, 0.1)\n",
    "\n",
    "        forecast_rows.append(new_row)\n",
    "        last_row = pd.Series(new_row)\n",
    "\n",
    "# === 6. Predict future avg_temp ===\n",
    "forecast_df = pd.DataFrame(forecast_rows)\n",
    "forecast_df = forecast_df.dropna(subset=features)\n",
    "forecast_df['predicted_avg_temp'] = model.predict(forecast_df[features])\n",
    "\n",
    "# === 7. Save results ===\n",
    "forecast_path = \"../data/processed/climate_forecast_2020_2050.csv\"\n",
    "forecast_df.to_csv(forecast_path, index=False)\n",
    "\n",
    "# === 8. Preview ===\n",
    "print(\"‚úÖ Climate Forecast Preview (2050):\")\n",
    "print(forecast_df[forecast_df['YEAR'] == 2050][['DISTRICT', 'YEAR', 'predicted_avg_temp']].round(2))\n",
    "print(f\"üíæ Forecast saved to: {forecast_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f2dcd-5947-404d-ad5d-66b862285e02",
   "metadata": {},
   "source": [
    "### ‚úÖ Glacier Area, Ice Volume, and Minimum Elevation Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e121c3-9075-4344-877d-06569f6f4a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import os\n",
    "\n",
    "# --- Step 0: Ensure output directory exists ---\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# --- Step 1: Copy glacier_long data ---\n",
    "# glacier_long = pd.read_csv(\"../data/processed/glacier_long.csv\")  # Uncomment if not already loaded\n",
    "df = glacier_long.copy()\n",
    "\n",
    "# --- Step 2: Encode categorical variables ---\n",
    "df['basin_code'] = df['basin'].astype('category').cat.codes\n",
    "df['subbasin_code'] = df['sub-basin'].astype('category').cat.codes\n",
    "\n",
    "# --- Step 3: Define input features and target variables ---\n",
    "features = ['year', 'basin_code', 'subbasin_code']\n",
    "targets = ['glacier_area', 'ice_volume', 'min_elev']\n",
    "\n",
    "# --- Step 4: Train one model per target ---\n",
    "models = {}\n",
    "for target in targets:\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    models[target] = model\n",
    "\n",
    "# --- Step 5: Generate forecast input combinations ---\n",
    "future_years = [2020, 2030, 2040, 2050]\n",
    "basin_info = df[['basin', 'sub-basin', 'basin_code', 'subbasin_code']].drop_duplicates()\n",
    "forecast_rows = []\n",
    "\n",
    "for year in future_years:\n",
    "    for _, row in basin_info.iterrows():\n",
    "        input_dict = {\n",
    "            'year': year,\n",
    "            'basin_code': row['basin_code'],\n",
    "            'subbasin_code': row['subbasin_code']\n",
    "        }\n",
    "        result = {\n",
    "            'year': year,\n",
    "            'basin': row['basin'],\n",
    "            'sub-basin': row['sub-basin']\n",
    "        }\n",
    "        for target in targets:\n",
    "            prediction = models[target].predict(pd.DataFrame([input_dict]))[0]\n",
    "            result[f'predicted_{target}'] = round(float(prediction), 4)\n",
    "        forecast_rows.append(result)\n",
    "\n",
    "# --- Step 6: Create forecast DataFrame ---\n",
    "forecast_df = pd.DataFrame(forecast_rows)\n",
    "\n",
    "# --- Step 7: Preview and save ---\n",
    "print(\"‚úÖ Glacier Forecast for 2050:\")\n",
    "print(forecast_df[forecast_df['year'] == 2050].round(2))\n",
    "\n",
    "forecast_path = \"../data/processed/glacier_forecast_2020_2050.csv\"\n",
    "forecast_df.to_csv(forecast_path, index=False)\n",
    "print(f\"üíæ Forecast saved to: {forecast_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

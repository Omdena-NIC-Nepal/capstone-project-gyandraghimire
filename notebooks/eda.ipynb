{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a53efc-24ee-4054-a601-cb0171895bc0",
   "metadata": {},
   "source": [
    "## ‚úÖ 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b2123-9737-4b72-8033-81236eba2edd",
   "metadata": {},
   "source": [
    "### ‚úÖ Temperature & Precipitation Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0470f-b19e-461f-833f-45fe3a228b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Load climate data\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# Step 2: Parse DATE and extract YEAR\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "\n",
    "# Step 3: Compute national yearly averages\n",
    "temp_precip = (\n",
    "    climate_df.groupby('YEAR')[['T2M', 'PRECTOT']]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 4: Plotting setup\n",
    "sns.set(style='whitegrid')\n",
    "fig, ax1 = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "# Plot average temperature on primary y-axis\n",
    "sns.lineplot(\n",
    "    data=temp_precip, x='YEAR', y='T2M',\n",
    "    label='Avg Temperature (¬∞C)', marker='o', ax=ax1, color='tab:red'\n",
    ")\n",
    "ax1.set_ylabel('Avg Temperature (¬∞C)', fontsize=12, color='tab:red')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# Plot average precipitation on secondary y-axis\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(\n",
    "    data=temp_precip, x='YEAR', y='PRECTOT',\n",
    "    label='Avg Precipitation (mm)', marker='o', ax=ax2, color='tab:blue'\n",
    ")\n",
    "ax2.set_ylabel('Avg Precipitation (mm)', fontsize=12, color='tab:blue')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Title and axis formatting\n",
    "ax1.set_xlabel('Year', fontsize=12)\n",
    "ax1.set_title('National Climate Trends in Nepal (1981‚Äì2019)', fontsize=14)\n",
    "ax1.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Merge legends from both axes\n",
    "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, title='Climate Variable', loc='upper left')\n",
    "\n",
    "# Finalize layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee016d4c-5054-4394-bff7-5f92bb47f2ff",
   "metadata": {},
   "source": [
    "### ‚úÖ Temperature Trends Across Regions and Elevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ea76ba-0412-4af8-b9a7-0e3c3a458564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Load the climate dataset\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# Step 2: Ensure DATE column is datetime and extract YEAR\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "\n",
    "# Step 3: Compute average annual temperature by district\n",
    "temp_trend = (\n",
    "    climate_df.groupby(['YEAR', 'DISTRICT'])['T2M']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 4: Sort for consistent line plotting\n",
    "temp_trend.sort_values(by=['DISTRICT', 'YEAR'], inplace=True)\n",
    "\n",
    "# Step 5: Plot temperature trends\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.lineplot(\n",
    "    data=temp_trend,\n",
    "    x='YEAR',\n",
    "    y='T2M',\n",
    "    hue='DISTRICT',\n",
    "    legend=False,  # Avoid cluttering legend if there are too many districts\n",
    "    palette='tab20',\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "plt.title('Average Annual Temperature Trend by District (1981‚Äì2019)', fontsize=14)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Temperature (¬∞C)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e8ed9-2e6e-4727-95fb-67648d04642d",
   "metadata": {},
   "source": [
    "### ‚úÖ Precipitation Patterns Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a19eb1a-b1e3-441e-b937-b0b5f3b19158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Load the climate dataset\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# Step 2: Ensure 'DATE' is in datetime format\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "\n",
    "# Step 3: Extract year\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "\n",
    "# Step 4: Group by year and compute total precipitation\n",
    "precip_trend = (\n",
    "    climate_df.groupby('YEAR', as_index=False)['PRECTOT']\n",
    "    .sum(min_count=1)  # Ensures NaNs don't default to zero\n",
    ")\n",
    "\n",
    "# Step 5: Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=precip_trend, x='YEAR', y='PRECTOT', marker='o', color='steelblue')\n",
    "\n",
    "plt.title('Total Annual Precipitation in Nepal (1981‚Äì2019)', fontsize=14)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Total Precipitation (mm)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e7e02d-78af-4f94-ac40-ab54475fab26",
   "metadata": {},
   "source": [
    "### ‚úÖ Extreme Weather Event Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272642c-2bc9-4f29-a6fd-45e5ad4f14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load the climate dataset\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# Step 2: Ensure 'DATE' is datetime\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "\n",
    "# Step 3: Extract year\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "\n",
    "# Step 4: Define extreme event thresholds\n",
    "heatwave_days = (\n",
    "    climate_df[climate_df['T2M_MAX'] > 35]\n",
    "    .groupby('YEAR')\n",
    "    .size()\n",
    "    .rename('Heatwave Days')\n",
    ")\n",
    "\n",
    "heavy_rain_days = (\n",
    "    climate_df[climate_df['PRECTOT'] > 50]\n",
    "    .groupby('YEAR')\n",
    "    .size()\n",
    "    .rename('Heavy Rain Days')\n",
    ")\n",
    "\n",
    "# Step 5: Merge into single DataFrame with all years represented\n",
    "all_years = pd.Series(range(climate_df['YEAR'].min(), climate_df['YEAR'].max() + 1), name='YEAR')\n",
    "extreme_df = pd.DataFrame(all_years)\n",
    "extreme_df = extreme_df.merge(heatwave_days, left_on='YEAR', right_index=True, how='left')\n",
    "extreme_df = extreme_df.merge(heavy_rain_days, left_on='YEAR', right_index=True, how='left')\n",
    "extreme_df.fillna(0, inplace=True)\n",
    "extreme_df[['Heatwave Days', 'Heavy Rain Days']] = extreme_df[['Heatwave Days', 'Heavy Rain Days']].astype(int)\n",
    "\n",
    "# Step 6: Plot the trends\n",
    "extreme_df.set_index('YEAR').plot(\n",
    "    kind='line',\n",
    "    marker='o',\n",
    "    figsize=(12, 6),\n",
    "    title='Annual Frequency of Extreme Weather Events in Nepal (1981‚Äì2019)'\n",
    ")\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Days')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9460a4-5930-4ff0-8d79-5c727c96c8f4",
   "metadata": {},
   "source": [
    "### ‚úÖ Correlation Between Climate Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa95d8-af93-405e-a053-d515dd29de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Load the climate dataset\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# Step 2: Select numeric climate variables\n",
    "variables = ['PRECTOT', 'T2M', 'RH2M', 'QV2M', 'WS10M']\n",
    "climate_subset = climate_df[variables].dropna()  # Remove rows with any missing values\n",
    "\n",
    "# Step 3: Compute correlation matrix\n",
    "climate_corr = climate_subset.corr()\n",
    "\n",
    "# Step 4: Plot correlation heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    climate_corr,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt=\".2f\",\n",
    "    linewidths=0.5,\n",
    "    square=True,\n",
    "    cbar_kws={\"shrink\": 0.8},\n",
    "    annot_kws={\"size\": 10}\n",
    ")\n",
    "\n",
    "plt.title('Correlation Matrix of Climate Variables', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86412e2-39fb-4072-a91e-9f6893b06045",
   "metadata": {},
   "source": [
    "### ‚úÖ Violin Plot: Temperature by Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbc7f2-571c-41f0-887a-32fb53108de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Load climate data\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# Step 2: Parse date and extract month\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['MONTH'] = climate_df['DATE'].dt.month\n",
    "\n",
    "# Step 3: Assign season if not already present\n",
    "def assign_season(month):\n",
    "    if pd.isna(month):\n",
    "        return None\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Autumn'\n",
    "    elif month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    return 'Unknown'\n",
    "\n",
    "climate_df['Season'] = climate_df['MONTH'].apply(assign_season)\n",
    "\n",
    "# Step 4: Filter clean rows for plotting\n",
    "climate_df = climate_df.dropna(subset=['Season', 'T2M'])\n",
    "\n",
    "# Step 5: Create violin plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.violinplot(\n",
    "    data=climate_df,\n",
    "    x='Season',\n",
    "    y='T2M',\n",
    "    hue='Season',           # Color by season\n",
    "    palette='Set2',\n",
    "    dodge=False,            # Overlay not split\n",
    "    legend=False            # Hue = x, no extra legend needed\n",
    ")\n",
    "\n",
    "plt.title('Distribution of Mean Temperature by Season', fontsize=14)\n",
    "plt.xlabel('Season', fontsize=12)\n",
    "plt.ylabel('Temperature (¬∞C)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30709618-d7f7-4781-bd70-17b830572487",
   "metadata": {},
   "source": [
    "### ‚úÖ Glacier Area, Volume, Elevation Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2bcbd8-d393-46c3-b494-a70ab7b3da24",
   "metadata": {},
   "source": [
    "#### 1. Average Glacier Area Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21eb65b-ee28-4026-804f-75916d553872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load glacier dataset\n",
    "glacier_path = \"../data/glaciers_change_in_basins_subbasins_1980_1990_2000_2010.csv\"\n",
    "glacier_df = pd.read_csv(glacier_path)\n",
    "\n",
    "# Step 2: Clean and normalize column names if needed\n",
    "glacier_df.columns = (\n",
    "    glacier_df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace('~', '', regex=False)\n",
    "    .str.replace(' ', '_')\n",
    "    .str.replace(r'\\(km2\\)', '', regex=True)\n",
    "    .str.replace(r'[()]', '', regex=True)\n",
    ")\n",
    "\n",
    "# Step 3: Compute average glacier area for each year\n",
    "avg_glacier_area = glacier_df[\n",
    "    ['glacier_area_in_1980', 'glacier_area_1990', 'glacier_area_2000', 'glacier_area_2010']\n",
    "].mean()\n",
    "\n",
    "# Step 4: Rename index for clarity in plot\n",
    "avg_glacier_area.index = ['1980', '1990', '2000', '2010']\n",
    "\n",
    "# Step 5: Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "avg_glacier_area.plot(marker='o', linestyle='-', color='teal')\n",
    "\n",
    "plt.title('Average Glacier Area Over Time (Nepal)', fontsize=14)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Glacier Area (km¬≤)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55adc12-489b-464e-a84c-d0cb23094e1b",
   "metadata": {},
   "source": [
    "#### 2. Average Glacier Ice Volume Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203130f-91e5-4a39-9251-64b078ec7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load glacier dataset\n",
    "glacier_path = \"../data/glaciers_change_in_basins_subbasins_1980_1990_2000_2010.csv\"\n",
    "glacier_df = pd.read_csv(glacier_path)\n",
    "\n",
    "# Step 2: Clean and normalize column names\n",
    "glacier_df.columns = (\n",
    "    glacier_df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace('~', '', regex=False)\n",
    "    .str.replace(' ', '_')\n",
    "    .str.replace(r'\\(km3\\)', '', regex=True)\n",
    "    .str.replace(r'[()]', '', regex=True)\n",
    ")\n",
    "\n",
    "# Step 3: Compute average glacier ice volume\n",
    "avg_glacier_volume = glacier_df[\n",
    "    ['estimated_ice_reserved_1980', 'estimated_ice_reserved_1990',\n",
    "     'estimated_ice_reserved2000', 'estimated_ice_reserved2010']\n",
    "].mean()\n",
    "\n",
    "# Step 4: Set year labels as index\n",
    "avg_glacier_volume.index = ['1980', '1990', '2000', '2010']\n",
    "\n",
    "# Step 5: Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "avg_glacier_volume.plot(marker='o', linestyle='-', color='teal')\n",
    "\n",
    "plt.title('Average Glacier Ice Volume Over Time (Nepal)', fontsize=14)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Ice Volume (km¬≥)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea73bf3-9bfc-44c2-a42d-72149391d6b1",
   "metadata": {},
   "source": [
    "#### 3. Average Minimum Glacier Elevation Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cfa529-ae39-4680-82d2-3878bee8bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load glacier dataset\n",
    "glacier_path = \"../data/glaciers_change_in_basins_subbasins_1980_1990_2000_2010.csv\"\n",
    "glacier_df = pd.read_csv(glacier_path)\n",
    "\n",
    "# Step 2: Clean and normalize column names\n",
    "glacier_df.columns = (\n",
    "    glacier_df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace('~', '', regex=False)\n",
    "    .str.replace(' ', '_')\n",
    "    .str.replace(r'\\(masl\\)', '', regex=True)\n",
    "    .str.replace(r'[()]', '', regex=True)\n",
    ")\n",
    "\n",
    "# Step 3: Compute average minimum elevation per year\n",
    "avg_min_elev = glacier_df[\n",
    "    ['minimum_elevation_in_1980', 'minimum_elevation_in1990',\n",
    "     'minimum_elevation_in2000', 'minimum_elevation_in2010']\n",
    "].mean()\n",
    "\n",
    "# Step 4: Set proper year labels\n",
    "avg_min_elev.index = ['1980', '1990', '2000', '2010']\n",
    "\n",
    "# Step 5: Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "avg_min_elev.plot(marker='o', linestyle='-', color='darkred')\n",
    "\n",
    "plt.title('Average Minimum Glacier Elevation Over Time (Nepal)', fontsize=14)\n",
    "plt.xlabel('Year', fontsize=12)\n",
    "plt.ylabel('Minimum Elevation (m a.s.l.)', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c551abd-e78d-482f-bad2-c2ed1349a562",
   "metadata": {},
   "source": [
    "### ‚úÖ Geospatial Map for Average Temperature by District"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd83bf9-1e70-4142-a571-5d5d1b261480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from fuzzywuzzy import process\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load data\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "shapefile_path = \"../data/local_unit_shapefiles/local_unit.shp\"\n",
    "\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Step 2: Standardize district names to uppercase\n",
    "climate_df['DISTRICT'] = climate_df['DISTRICT'].str.strip().str.upper()\n",
    "gdf['DISTRICT'] = gdf['DISTRICT'].str.strip().str.upper()\n",
    "\n",
    "# Step 3: Compute average temperature per district\n",
    "avg_temp = climate_df.groupby('DISTRICT', as_index=False)['T2M'].mean()\n",
    "\n",
    "# Step 4: Fuzzy match district names\n",
    "climate_districts = avg_temp['DISTRICT'].unique()\n",
    "\n",
    "gdf['DISTRICT_CLEAN'] = gdf['DISTRICT'].apply(\n",
    "    lambda x: process.extractOne(x, climate_districts)[0] if pd.notnull(x) else None\n",
    ")\n",
    "\n",
    "# Step 5: Dissolve geometries by matched names\n",
    "gdf_dissolved = gdf.dissolve(by='DISTRICT_CLEAN', as_index=False)\n",
    "\n",
    "# Step 6: Filter only matched districts\n",
    "gdf_matched = gdf_dissolved[gdf_dissolved['DISTRICT_CLEAN'].isin(avg_temp['DISTRICT'])]\n",
    "\n",
    "# Step 7: Merge temperature data\n",
    "gdf_final = gdf_matched.merge(avg_temp, left_on='DISTRICT_CLEAN', right_on='DISTRICT', how='left')\n",
    "\n",
    "# Step 8: Clean up invalid geometries\n",
    "gdf_final = gdf_final.dropna(subset=['T2M'])\n",
    "gdf_final = gdf_final[gdf_final.geometry.notnull() & gdf_final.is_valid & ~gdf_final.is_empty]\n",
    "\n",
    "# Step 9: Plot choropleth\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "gdf_final.plot(\n",
    "    column='T2M',\n",
    "    cmap='OrRd',\n",
    "    legend=True,\n",
    "    ax=ax,\n",
    "    edgecolor='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "ax.set_title('Average Temperature by District (1981‚Äì2019)', fontsize=14)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a1f25-8253-495b-8a16-2ac16b8c0651",
   "metadata": {},
   "source": [
    "### Diagnostics for Spatial Join and Fuzzy Matching Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d7071b-5743-45ce-a7d2-7dab0f834298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Count of matched districts with valid geometry\n",
    "matched_district_count = gdf_final['DISTRICT_CLEAN'].nunique()\n",
    "print(f\"‚úÖ Matched districts with geometry: {matched_district_count}\")\n",
    "\n",
    "# Step 2: Total unique districts in the climate dataset\n",
    "total_climate_districts = avg_temp['DISTRICT'].nunique()\n",
    "print(f\"üìä Total unique districts in climate data: {total_climate_districts}\")\n",
    "\n",
    "# Step 3: Report unmatched district count\n",
    "unmatched = total_climate_districts - matched_district_count\n",
    "print(f\"‚ùó Unmatched districts (likely due to name mismatch or missing geometry): {unmatched}\")\n",
    "\n",
    "# Step 4: List unmatched districts\n",
    "matched_set = set(gdf_final['DISTRICT_CLEAN'].dropna())\n",
    "all_set = set(avg_temp['DISTRICT'].dropna())\n",
    "unmatched_districts = sorted(all_set - matched_set)\n",
    "\n",
    "# Output\n",
    "if unmatched_districts:\n",
    "    print(\"üîç Unmatched Districts:\")\n",
    "    for dist in unmatched_districts:\n",
    "        print(f\"  - {dist}\")\n",
    "else:\n",
    "    print(\"‚úÖ All districts matched successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867fec2-9bd8-4953-915b-285123f90b1b",
   "metadata": {},
   "source": [
    "### ‚úÖ Choropleth map for average maximum temperature (T2M_MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38603b1d-3c87-4a7d-b9bb-65ac260e9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from fuzzywuzzy import process\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Step 1: Load Data ===\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "shapefile_path = \"../data/local_unit_shapefiles/local_unit.shp\"\n",
    "\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# === Step 2: Standardize District Names ===\n",
    "climate_df['DISTRICT'] = climate_df['DISTRICT'].str.strip().str.upper()\n",
    "gdf['DISTRICT'] = gdf['DISTRICT'].str.strip().str.upper()\n",
    "\n",
    "# === Step 3: Compute District-wise Climate Averages ===\n",
    "avg_temp = (\n",
    "    climate_df.groupby('DISTRICT', as_index=False)[['T2M', 'T2M_MAX']]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# === Step 4: Fuzzy Match Shapefile Districts to Climate Districts ===\n",
    "climate_districts = avg_temp['DISTRICT'].unique()\n",
    "gdf['DISTRICT_CLEAN'] = gdf['DISTRICT'].apply(\n",
    "    lambda x: process.extractOne(x, climate_districts)[0] if pd.notnull(x) else None\n",
    ")\n",
    "\n",
    "# === Step 5: Dissolve Local Units into Districts ===\n",
    "gdf_dissolved = gdf.dissolve(by='DISTRICT_CLEAN', as_index=False)\n",
    "\n",
    "# === Step 6: Filter to Matched Districts ===\n",
    "gdf_matched = gdf_dissolved[gdf_dissolved['DISTRICT_CLEAN'].isin(avg_temp['DISTRICT'])]\n",
    "\n",
    "# === Step 7: Merge with Climate Averages ===\n",
    "gdf_final = gdf_matched.merge(\n",
    "    avg_temp, left_on='DISTRICT_CLEAN', right_on='DISTRICT', how='left'\n",
    ")\n",
    "\n",
    "# === Step 8: Clean Geometries ===\n",
    "gdf_final = gdf_final.dropna(subset=['T2M', 'T2M_MAX'])\n",
    "gdf_final = gdf_final[gdf_final.geometry.notnull() & gdf_final.is_valid & ~gdf_final.is_empty]\n",
    "\n",
    "# === Step 9: Choropleth Plot of T2M_MAX ===\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "gdf_final.plot(\n",
    "    column='T2M_MAX',\n",
    "    cmap='YlOrRd',\n",
    "    legend=True,\n",
    "    ax=ax,\n",
    "    edgecolor='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "ax.set_title('Average Maximum Temperature by District (1981‚Äì2019)', fontsize=14)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === Step 10: Matching Diagnostics ===\n",
    "matched_district_count = gdf_final['DISTRICT_CLEAN'].nunique()\n",
    "total_climate_districts = avg_temp['DISTRICT'].nunique()\n",
    "unmatched = total_climate_districts - matched_district_count\n",
    "\n",
    "print(f\"\\n‚úÖ Matched districts: {matched_district_count}\")\n",
    "print(f\"üìä Total districts in climate data: {total_climate_districts}\")\n",
    "print(f\"‚ùó Unmatched districts: {unmatched}\")\n",
    "\n",
    "unmatched_districts = sorted(set(avg_temp['DISTRICT']) - set(gdf_final['DISTRICT_CLEAN']))\n",
    "if unmatched_districts:\n",
    "    print(\"üîç Unmatched Districts:\")\n",
    "    for dist in unmatched_districts:\n",
    "        print(f\"  - {dist}\")\n",
    "else:\n",
    "    print(\"‚úÖ All districts matched successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d0c4d-534b-422b-aacc-4da7cf6d37a0",
   "metadata": {},
   "source": [
    "### ‚úÖ Geospatial Map for Average Precipitation by District"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a992084b-6ca7-4a6d-bb1c-f7416bf64eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from fuzzywuzzy import process\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Step 1: Load Data ===\n",
    "climate_path = \"../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz\"\n",
    "shapefile_path = \"../data/local_unit_shapefiles/local_unit.shp\"\n",
    "\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# === Step 2: Standardize District Names ===\n",
    "climate_df['DISTRICT'] = climate_df['DISTRICT'].str.strip().str.upper()\n",
    "gdf['DISTRICT'] = gdf['DISTRICT'].str.strip().str.upper()\n",
    "\n",
    "# === Step 3: Compute Average Annual Precipitation per District ===\n",
    "avg_precip = climate_df.groupby('DISTRICT', as_index=False)['PRECTOT'].mean()\n",
    "\n",
    "# === Step 4: Fuzzy Match Climate Districts to Shapefile ===\n",
    "climate_districts = avg_precip['DISTRICT'].unique()\n",
    "gdf['DISTRICT_CLEAN'] = gdf['DISTRICT'].apply(\n",
    "    lambda x: process.extractOne(x, climate_districts)[0] if pd.notnull(x) else None\n",
    ")\n",
    "\n",
    "# === Step 5: Dissolve by Clean District Names ===\n",
    "gdf_dissolved = gdf.dissolve(by='DISTRICT_CLEAN', as_index=False)\n",
    "\n",
    "# === Step 6: Filter to Climate-Matched Districts ===\n",
    "gdf_matched = gdf_dissolved[gdf_dissolved['DISTRICT_CLEAN'].isin(avg_precip['DISTRICT'])]\n",
    "\n",
    "# === Step 7: Merge Average Precipitation Data ===\n",
    "gdf_precip = gdf_matched.merge(avg_precip, left_on='DISTRICT_CLEAN', right_on='DISTRICT', how='left')\n",
    "\n",
    "# === Step 8: Drop Invalid or Missing Geometries ===\n",
    "gdf_precip = gdf_precip.dropna(subset=['PRECTOT'])\n",
    "gdf_precip = gdf_precip[\n",
    "    gdf_precip.geometry.notnull() & gdf_precip.is_valid & ~gdf_precip.is_empty\n",
    "]\n",
    "\n",
    "# === Step 9: Plot Choropleth ===\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "gdf_precip.plot(\n",
    "    column='PRECTOT',\n",
    "    cmap='Blues',\n",
    "    legend=True,\n",
    "    ax=ax,\n",
    "    edgecolor='black',\n",
    "    linewidth=0.5\n",
    ")\n",
    "\n",
    "ax.set_title('Average Annual Precipitation by District (1981‚Äì2019)', fontsize=14)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762301d-c485-4d42-9493-171a958dff07",
   "metadata": {},
   "source": [
    "### ‚úÖ Interactive Choropleth Map for Average Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a60a75-028d-4a24-9429-367e7001bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium import Choropleth\n",
    "import geopandas as gpd\n",
    "\n",
    "# Step 1: Reproject to WGS84 (EPSG:4326) for web mapping\n",
    "gdf_final = gdf_final.to_crs(epsg=4326)\n",
    "\n",
    "# Step 2: Prepare columns for mapping\n",
    "gdf_final['DISTRICT_CLEAN'] = gdf_final['DISTRICT_CLEAN'].astype(str)\n",
    "gdf_final['T2M'] = gdf_final['T2M'].round(2)  # Round for display\n",
    "\n",
    "# Step 3: Initialize Folium map centered on Nepal\n",
    "m = folium.Map(location=[28.3, 84.0], zoom_start=7, tiles='CartoDB positron')\n",
    "\n",
    "# Step 4: Add choropleth layer\n",
    "Choropleth(\n",
    "    geo_data=gdf_final,\n",
    "    data=gdf_final,\n",
    "    columns=['DISTRICT_CLEAN', 'T2M'],\n",
    "    key_on='feature.properties.DISTRICT_CLEAN',\n",
    "    fill_color='OrRd',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.5,\n",
    "    legend_name='Average Temperature (¬∞C)',\n",
    "    highlight=True,\n",
    "    nan_fill_color='gray'\n",
    ").add_to(m)\n",
    "\n",
    "# Step 5: Add tooltips to each district\n",
    "folium.GeoJson(\n",
    "    gdf_final,\n",
    "    name=\"District Labels\",\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=[\"DISTRICT_CLEAN\", \"T2M\"],\n",
    "        aliases=[\"District\", \"Temperature (¬∞C)\"],\n",
    "        localize=True,\n",
    "        sticky=True,\n",
    "        labels=True,\n",
    "        style=\"\"\"\n",
    "            background-color: white;\n",
    "            color: #333;\n",
    "            font-family: Arial;\n",
    "            font-size: 12px;\n",
    "            padding: 5px;\n",
    "        \"\"\"\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Step 6: Add layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Step 7: Save interactive map to file\n",
    "m.save(\"average_temperature_map.html\")\n",
    "\n",
    "# Step 8: Display map in Jupyter notebook (if supported)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5619fd-0570-4cbc-97b5-52923924cd89",
   "metadata": {},
   "source": [
    "### ‚úÖ Interactive Choropleth Map for Average Precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cee305-6038-4226-bef5-994005db8da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium import Choropleth\n",
    "import geopandas as gpd\n",
    "\n",
    "# Step 1: Reproject GeoDataFrame to WGS84 (EPSG:4326) for web mapping\n",
    "gdf_precip = gdf_precip.to_crs(epsg=4326)\n",
    "\n",
    "# Step 2: Ensure clean column data for JSON export\n",
    "gdf_precip['DISTRICT_CLEAN'] = gdf_precip['DISTRICT_CLEAN'].astype(str)\n",
    "gdf_precip['PRECTOT'] = gdf_precip['PRECTOT'].round(2)\n",
    "\n",
    "# Step 3: Initialize Folium map centered on Nepal\n",
    "m = folium.Map(location=[28.3, 84.0], zoom_start=7, tiles='CartoDB positron')\n",
    "\n",
    "# Step 4: Add choropleth layer\n",
    "Choropleth(\n",
    "    geo_data=gdf_precip,\n",
    "    data=gdf_precip,\n",
    "    columns=['DISTRICT_CLEAN', 'PRECTOT'],\n",
    "    key_on='feature.properties.DISTRICT_CLEAN',\n",
    "    fill_color='YlGnBu',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.4,\n",
    "    legend_name='Average Precipitation (mm)',\n",
    "    highlight=True,\n",
    "    nan_fill_color='gray'\n",
    ").add_to(m)\n",
    "\n",
    "# Step 5: Add GeoJSON with tooltips\n",
    "folium.GeoJson(\n",
    "    gdf_precip,\n",
    "    name=\"District Info\",\n",
    "    tooltip=folium.GeoJsonTooltip(\n",
    "        fields=[\"DISTRICT_CLEAN\", \"PRECTOT\"],\n",
    "        aliases=[\"District\", \"Precipitation (mm)\"],\n",
    "        localize=True,\n",
    "        labels=True,\n",
    "        sticky=True,\n",
    "        style=\"\"\"\n",
    "            background-color: white;\n",
    "            color: #333;\n",
    "            font-family: Arial;\n",
    "            font-size: 12px;\n",
    "            padding: 5px;\n",
    "        \"\"\"\n",
    "    )\n",
    ").add_to(m)\n",
    "\n",
    "# Step 6: Add layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Step 7: Save map to file\n",
    "m.save('avg_precipitation_map.html')\n",
    "\n",
    "# Step 8: Display in Jupyter (if supported)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb6559-bdd3-4aa6-938c-fad6183723aa",
   "metadata": {},
   "source": [
    "### ‚úÖ 7. Plotly Line Chart for Temperature Trends by District"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62080779-040d-4cee-afbc-703595f5e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Step 1: Normalize column names (in case of prior inconsistencies)\n",
    "temp_trend.columns = temp_trend.columns.str.strip().str.upper()\n",
    "\n",
    "# Step 2: Rename columns for clarity in plot\n",
    "temp_trend = temp_trend.rename(columns={\n",
    "    'T2M': 'Temperature',\n",
    "    'YEAR': 'Year',\n",
    "    'DISTRICT': 'District'\n",
    "})\n",
    "\n",
    "# Step 3: Generate interactive line chart\n",
    "fig = px.line(\n",
    "    temp_trend,\n",
    "    x='Year',\n",
    "    y='Temperature',\n",
    "    color='District',\n",
    "    title='Average Temperature Trend by District (1981‚Äì2019)',\n",
    "    labels={\n",
    "        'Temperature': 'Temperature (¬∞C)',\n",
    "        'Year': 'Year',\n",
    "        'District': 'District'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Step 4: Update layout for styling\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    legend_title_text='District',\n",
    "    margin=dict(l=40, r=40, t=60, b=40),\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Step 5: Display figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b1349-eeb3-4aa1-9125-d4c623821e7b",
   "metadata": {},
   "source": [
    "### ‚úÖ Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc72317-5f80-47d5-b0da-714aab0216ef",
   "metadata": {},
   "source": [
    "#### ‚úÖ Statistical Test for Temperature Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da974a2-5d77-45a1-8dce-9d5a7465c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Step 1: Ensure DATE column is datetime and extract YEAR\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "\n",
    "# Step 2: Drop rows with missing temperature or year\n",
    "climate_df_clean = climate_df.dropna(subset=['T2M', 'YEAR'])\n",
    "\n",
    "# Step 3: Split into groups: before and after 2000\n",
    "before_2000 = climate_df_clean[climate_df_clean['YEAR'] < 2000]['T2M']\n",
    "after_2000 = climate_df_clean[climate_df_clean['YEAR'] >= 2000]['T2M']\n",
    "\n",
    "# Step 4: Perform Welch's t-test (unequal variances)\n",
    "t_stat, p_value = ttest_ind(before_2000, after_2000, equal_var=False)\n",
    "\n",
    "# Step 5: Report results\n",
    "print(\"üß™ Welch's T-Test: Mean Temperature Before vs After 2000\")\n",
    "print(f\"t-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.5f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"‚úÖ Statistically significant difference in mean temperature.\")\n",
    "else:\n",
    "    print(\"‚ùå No statistically significant difference in mean temperature.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904bfe00-003d-490d-92c4-2f4a83edbe4d",
   "metadata": {},
   "source": [
    "#### ‚úÖ Statistical Test for Precipitation Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef991774-7233-4cb5-8908-d64eca230bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Step 1: Ensure DATE is datetime and extract YEAR\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "\n",
    "# Step 2: Clean data ‚Äî drop rows with missing precipitation or year\n",
    "climate_df_clean = climate_df.dropna(subset=['PRECTOT', 'YEAR'])\n",
    "\n",
    "# Step 3: Split into groups: before and after 2000\n",
    "precip_before_2000 = climate_df_clean[climate_df_clean['YEAR'] < 2000]['PRECTOT']\n",
    "precip_after_2000 = climate_df_clean[climate_df_clean['YEAR'] >= 2000]['PRECTOT']\n",
    "\n",
    "# Step 4: Perform Welch‚Äôs t-test (unequal variances)\n",
    "t_stat, p_value = ttest_ind(precip_before_2000, precip_after_2000, equal_var=False)\n",
    "\n",
    "# Step 5: Output results\n",
    "print(\"\\nüß™ Welch's T-Test: Mean Precipitation Before vs After 2000\")\n",
    "print(f\"t-statistic: {t_stat:.3f}\")\n",
    "print(f\"p-value: {p_value:.5f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"‚úÖ Statistically significant difference in mean precipitation.\")\n",
    "else:\n",
    "    print(\"‚ùå No statistically significant difference in mean precipitation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff634d70-86de-4873-a76f-cd39c64b8d5d",
   "metadata": {},
   "source": [
    "#### ‚úÖ Correlation Tests (Temperature vs Precipitation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaf2f37-6f52-4636-a2a4-365fe9ce5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# --- 1. Define variable pairs to test ---\n",
    "var_pairs = [\n",
    "    ('T2M', 'PRECTOT'),       # Temperature vs Precipitation\n",
    "    ('T2M', 'RH2M'),          # Temperature vs Relative Humidity\n",
    "    ('T2M', 'QV2M'),          # Temperature vs Specific Humidity\n",
    "    ('T2M', 'WS10M'),         # Temperature vs Wind Speed\n",
    "    ('T2M_MAX', 'PRECTOT'),   # Max Temperature vs Precipitation\n",
    "    ('T2M_RANGE', 'PRECTOT')  # Temp Range vs Precipitation\n",
    "]\n",
    "\n",
    "# --- 2. Run correlation tests ---\n",
    "print(\"\\nüîó Correlation Analysis:\")\n",
    "header = f\"{'Variable Pair':<30} {'Pearson r':>10} {'p':>10} {'Status':>14} | {'Spearman œÅ':>10} {'p':>10} {'Status':>14}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "\n",
    "for x, y in var_pairs:\n",
    "    # Drop NaNs and align both series\n",
    "    pair_df = climate_df[[x, y]].dropna()\n",
    "\n",
    "    # Skip if not enough data\n",
    "    if pair_df.shape[0] < 3:\n",
    "        print(f\"{x} vs {y:<20} {'-':>10} {'-':>10} {'Insufficient':>14} | {'-':>10} {'-':>10} {'data':>14}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        # Compute correlations\n",
    "        r, p_r = pearsonr(pair_df[x], pair_df[y])\n",
    "        rho, p_rho = spearmanr(pair_df[x], pair_df[y])\n",
    "\n",
    "        # Interpret significance\n",
    "        pearson_status = \"‚úÖ Correlated\" if p_r < 0.05 else \"‚ùå Not\"\n",
    "        spearman_status = \"‚úÖ Correlated\" if p_rho < 0.05 else \"‚ùå Not\"\n",
    "\n",
    "        print(f\"{x} vs {y:<20} {r:10.2f} {p_r:10.4f} {pearson_status:>14} | {rho:10.2f} {p_rho:10.4f} {spearman_status:>14}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{x} vs {y:<20} {'-':>10} {'-':>10} {'Error':>14} | {'-':>10} {'-':>10} {'Error':>14}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385f9ca1-5d77-4fe6-8e22-b13f2af7065f",
   "metadata": {},
   "source": [
    "#### ‚úÖ ANOVA (Temperature difference between Seasons) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0839f4-4a8c-4b89-860e-d283ac3aa86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Ensure DATE is datetime\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "\n",
    "# Extract MONTH if missing\n",
    "if 'MONTH' not in climate_df.columns:\n",
    "    climate_df['MONTH'] = climate_df['DATE'].dt.month\n",
    "\n",
    "# Define seasons\n",
    "def get_season(month):\n",
    "    if pd.isna(month):\n",
    "        return None\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Autumn'\n",
    "    elif month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    return 'Unknown'\n",
    "\n",
    "# Add or replace Season column\n",
    "climate_df['Season'] = climate_df['MONTH'].apply(get_season)\n",
    "\n",
    "# Drop rows with missing values in T2M or Season\n",
    "climate_df_clean = climate_df.dropna(subset=['T2M', 'Season'])\n",
    "\n",
    "# Group by season\n",
    "season_groups = [\n",
    "    group['T2M'].values\n",
    "    for _, group in climate_df_clean.groupby('Season')\n",
    "    if group['T2M'].notnull().sum() > 1\n",
    "]\n",
    "\n",
    "# Check if enough groups are available\n",
    "if len(season_groups) < 2:\n",
    "    print(\"‚ö†Ô∏è Not enough valid seasonal groups for ANOVA.\")\n",
    "else:\n",
    "    f_stat, p_val = f_oneway(*season_groups)\n",
    "    result = \"‚úÖ Significant\" if p_val < 0.05 else \"‚ùå Not Significant\"\n",
    "\n",
    "    print(\"üß™ One-Way ANOVA: Temperature Differences Across Seasons\")\n",
    "    print(f\"F-statistic: {f_stat:.2f}\")\n",
    "    print(f\"p-value    : {p_val:.4f}\")\n",
    "    print(f\"Result     : {result} difference in mean temperature across seasons.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38a173-fa2a-4e92-9409-7ebe27c0bf9d",
   "metadata": {},
   "source": [
    "#### ‚úÖ Mann-Kendall Trend Test (Temperature, Precipitation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1d1ac1-b81f-4845-922c-92390557154b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymannkendall as mk\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Ensure DATE is datetime and YEAR is extracted\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "\n",
    "# --- Annual Mean Temperature ---\n",
    "annual_temp = climate_df.groupby('YEAR')['T2M'].mean().dropna().sort_index()\n",
    "\n",
    "if len(annual_temp) >= 10:\n",
    "    temp_result = mk.original_test(annual_temp)\n",
    "    print(\"üìà Mann-Kendall Test ‚Äì Annual Mean Temperature\")\n",
    "    print(f\"Trend     : {temp_result.trend}\")\n",
    "    print(f\"p-value   : {temp_result.p:.5f}\")\n",
    "    print(f\"Slope     : {temp_result.slope:.5f}\")\n",
    "    print(f\"Decision  : {'‚úÖ Significant trend detected' if temp_result.h else '‚ùå No significant trend'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough data points for temperature trend analysis.\")\n",
    "\n",
    "# --- Annual Mean Precipitation ---\n",
    "annual_precip = climate_df.groupby('YEAR')['PRECTOT'].mean().dropna().sort_index()\n",
    "\n",
    "if len(annual_precip) >= 10:\n",
    "    precip_result = mk.original_test(annual_precip)\n",
    "    print(\"\\nüåßÔ∏è Mann-Kendall Test ‚Äì Annual Mean Precipitation\")\n",
    "    print(f\"Trend     : {precip_result.trend}\")\n",
    "    print(f\"p-value   : {precip_result.p:.5f}\")\n",
    "    print(f\"Slope     : {precip_result.slope:.5f}\")\n",
    "    print(f\"Decision  : {'‚úÖ Significant trend detected' if precip_result.h else '‚ùå No significant trend'}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Not enough data points for precipitation trend analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4519b92-cad5-4528-a73f-df7c13301b61",
   "metadata": {},
   "source": [
    "### ‚úÖ Mann-Kendall Test for Annual Mean T2M_MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c85799-8215-4a78-83ff-a548a1d22d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymannkendall as mk\n",
    "\n",
    "# Step 1: Ensure DATE and YEAR\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "\n",
    "# Step 2: Group by YEAR and compute mean T2M_MAX\n",
    "annual_max_temp = climate_df.groupby('YEAR')['T2M_MAX'].mean().dropna().sort_index()\n",
    "\n",
    "# Step 3: Run Mann-Kendall test if enough data\n",
    "if len(annual_max_temp) >= 10:\n",
    "    t2mmax_result = mk.original_test(annual_max_temp)\n",
    "    print(\"üìà Mann-Kendall Test ‚Äì Annual Mean Maximum Temperature (T2M_MAX)\")\n",
    "    print(f\"Trend     : {t2mmax_result.trend}\")\n",
    "    print(f\"p-value   : {t2mmax_result.p:.5f}\")\n",
    "    print(f\"Slope     : {t2mmax_result.slope:.5f}\")\n",
    "    print(f\"Decision  : {'‚úÖ Significant trend detected' if t2mmax_result.h else '‚ùå No significant trend'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough data points for T2M_MAX trend analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c62551-ece6-41da-8cd0-912b65245d84",
   "metadata": {},
   "source": [
    "### ‚úÖ Mann-Kendall Test for T2M_MIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560465d6-b5a9-4016-a6df-9a7656aba74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymannkendall as mk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Compute annual mean minimum temperature\n",
    "annual_min_temp = climate_df.groupby('YEAR')['T2M_MIN'].mean().dropna().sort_index()\n",
    "\n",
    "# Step 2: Run Mann-Kendall test\n",
    "if len(annual_min_temp) >= 10:\n",
    "    t2mmin_result = mk.original_test(annual_min_temp)\n",
    "    print(\"üåô Mann-Kendall Test ‚Äì Annual Mean Minimum Temperature (T2M_MIN)\")\n",
    "    print(f\"Trend     : {t2mmin_result.trend}\")\n",
    "    print(f\"p-value   : {t2mmin_result.p:.5f}\")\n",
    "    print(f\"Slope     : {t2mmin_result.slope:.5f}\")\n",
    "    print(f\"Decision  : {'‚úÖ Significant trend detected' if t2mmin_result.h else '‚ùå No significant trend'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough data points for T2M_MIN trend analysis.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7a08a-a46e-41d4-b854-398773d9d8be",
   "metadata": {},
   "source": [
    "### ‚úÖ Trend in Daily Temperature Range (T2M_RANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b2708b-2f6c-4c3a-800e-d3b31a3e62d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_range = climate_df.groupby('YEAR')['T2M_RANGE'].mean().dropna().sort_index()\n",
    "range_result = mk.original_test(annual_range)\n",
    "\n",
    "print(\"\\nüåó Mann-Kendall Test ‚Äì Annual Temperature Range (T2M_MAX - T2M_MIN)\")\n",
    "print(f\"Trend     : {range_result.trend}\")\n",
    "print(f\"p-value   : {range_result.p:.5f}\")\n",
    "print(f\"Slope     : {range_result.slope:.5f}\")\n",
    "print(f\"Decision  : {'‚úÖ Significant trend detected' if range_result.h else '‚ùå No significant trend'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f472c75-72d9-47c0-a0d9-661ac70efd11",
   "metadata": {},
   "source": [
    "### ‚úÖ Trend in Daily Temperature Range (T2M_RANGE) by District"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452514b1-1eb8-46df-94a1-a32154a39dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymannkendall as mk\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Step 1: Ensure DATE and YEAR are available ===\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], errors='coerce')\n",
    "climate_df['YEAR'] = climate_df['DATE'].dt.year\n",
    "\n",
    "# === Step 2: Compute annual average T2M_RANGE per district ===\n",
    "annual_range_by_dist = (\n",
    "    climate_df\n",
    "    .dropna(subset=['YEAR', 'DISTRICT', 'T2M_RANGE'])\n",
    "    .groupby(['DISTRICT', 'YEAR'])['T2M_RANGE']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# === Step 3: Run Mann-Kendall trend test per district ===\n",
    "district_trends = []\n",
    "\n",
    "for district, group in annual_range_by_dist.groupby('DISTRICT'):\n",
    "    series = group.sort_values('YEAR')['T2M_RANGE']\n",
    "    if len(series) >= 10:\n",
    "        result = mk.original_test(series)\n",
    "        district_trends.append({\n",
    "            'DISTRICT': district,\n",
    "            'trend': result.trend,\n",
    "            'p_value': result.p,\n",
    "            'slope': result.slope,\n",
    "            'significant': int(result.h)\n",
    "        })\n",
    "\n",
    "# === Step 4: Create trend DataFrame ===\n",
    "trend_df = pd.DataFrame(district_trends)\n",
    "trend_df['DISTRICT'] = trend_df['DISTRICT'].str.strip().str.upper()\n",
    "\n",
    "# === Step 5: Load and prep GeoDataFrame if not preloaded ===\n",
    "try:\n",
    "    gdf_districts\n",
    "except NameError:\n",
    "    shapefile_path = \"../data/local_unit_shapefiles/local_unit.shp\"\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    gdf['DISTRICT'] = gdf['DISTRICT'].str.strip().str.upper()\n",
    "    gdf_districts = gdf.dissolve(by='DISTRICT', as_index=False)\n",
    "\n",
    "# === Step 6: Merge trend results into GeoDataFrame ===\n",
    "gdf_range_trend = gdf_districts.merge(trend_df, on='DISTRICT', how='left')\n",
    "\n",
    "# === Step 7: Plot slope of T2M_RANGE trend per district ===\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "gdf_range_trend.plot(\n",
    "    column='slope',\n",
    "    cmap='coolwarm_r',\n",
    "    legend=True,\n",
    "    ax=ax,\n",
    "    edgecolor='black',\n",
    "    linewidth=0.5,\n",
    "    missing_kwds={'color': 'lightgrey', 'label': 'No data'}\n",
    ")\n",
    "\n",
    "ax.set_title(\"Trend in Daily Temperature Range (T2M_RANGE) by District\", fontsize=14)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bb9fca-76ed-42da-a574-6758ef2f6ea5",
   "metadata": {},
   "source": [
    "#### ‚úÖ Chi-Square Test (Extreme Event and Season) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c552cd87-5504-468d-9ae6-7db99491e62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Step 1: Drop rows with missing values and create a clean copy\n",
    "climate_df_clean = climate_df.dropna(subset=['T2M_MAX', 'Season', 'YEAR']).copy()\n",
    "\n",
    "# Step 2: Create binary column: Extreme Heat (T2M_MAX > 35¬∞C)\n",
    "climate_df_clean['ExtremeHeat'] = (climate_df_clean['T2M_MAX'] > 35).astype(int)\n",
    "\n",
    "# --- Chi-square Test: Season vs Extreme Heat ---\n",
    "season_ct = pd.crosstab(climate_df_clean['Season'], climate_df_clean['ExtremeHeat'])\n",
    "chi2_season, p_season, dof_season, expected_season = chi2_contingency(season_ct)\n",
    "season_status = \"‚úÖ Significant\" if p_season < 0.05 else \"‚ùå Not Significant\"\n",
    "\n",
    "print(\"üìä Chi-square Test ‚Äì Season vs Extreme Heat\")\n",
    "print(f\"œá¬≤ = {chi2_season:.2f}, df = {dof_season}, p = {p_season:.4f} ‚Üí {season_status}\")\n",
    "\n",
    "# --- Chi-square Test: Year vs Extreme Heat ---\n",
    "year_ct = pd.crosstab(climate_df_clean['YEAR'], climate_df_clean['ExtremeHeat'])\n",
    "chi2_year, p_year, dof_year, expected_year = chi2_contingency(year_ct)\n",
    "year_status = \"‚úÖ Significant\" if p_year < 0.05 else \"‚ùå Not Significant\"\n",
    "\n",
    "print(\"\\nüìä Chi-square Test ‚Äì Year vs Extreme Heat\")\n",
    "print(f\"œá¬≤ = {chi2_year:.2f}, df = {dof_year}, p = {p_year:.4f} ‚Üí {year_status}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cf2e50-2c6f-422a-9284-b75d6297cfb7",
   "metadata": {},
   "source": [
    "#### ‚úÖ OLS Regression (Predict precipitation from temperature/humidity/etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27a7444-a194-47bd-a51c-c0f6b4b0557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Drop rows with missing values in predictors or response\n",
    "regression_df = climate_df[['T2M', 'RH2M', 'PRECTOT']].dropna().copy()\n",
    "\n",
    "# Step 2: Define predictors (X) and response variable (y)\n",
    "X = regression_df[['T2M', 'RH2M']]   # Independent variables\n",
    "y = regression_df['PRECTOT']         # Dependent variable\n",
    "\n",
    "# Step 3: Add constant term to predictors (intercept)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Step 4: Fit the Ordinary Least Squares (OLS) regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Step 5: Print regression summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416c866-f5c4-4ce7-b69e-04af71746794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Step 1: Clean and define your features\n",
    "extreme_df = climate_df[['PRECTOT', 'T2M', 'RH2M']].dropna().copy()\n",
    "\n",
    "# Step 2: Create binary target for heavy rainfall (1 = extreme, 0 = not)\n",
    "extreme_df['HeavyRain'] = (extreme_df['PRECTOT'] > 50).astype(int)\n",
    "\n",
    "# Step 3: Define predictors and response\n",
    "X = extreme_df[['T2M', 'RH2M']]      # Features\n",
    "y = extreme_df['HeavyRain']          # Binary target\n",
    "\n",
    "# Step 4: Add intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Step 5: Fit logistic regression model\n",
    "logit_model = sm.Logit(y, X).fit()\n",
    "\n",
    "# Step 6: Output model summary\n",
    "print(logit_model.summary())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

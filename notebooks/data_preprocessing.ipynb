{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58541486-5a6f-490d-8738-6cfeda08dff3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## ‚úÖ 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88b5c2-9f3c-4622-bde0-027f907d6181",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### ‚úÖ Load and Preview Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a917cf-4ca3-473f-9a4e-0cacf1707195",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the base data path\n",
    "data_path = Path(\"../data\")\n",
    "\n",
    "# Function to load CSV safely\n",
    "def load_csv_safe(filepath, **kwargs):\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, **kwargs)\n",
    "        print(f\"‚úÖ Loaded: {filepath.name} with shape {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found: {filepath}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {filepath.name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Function to load shapefile safely\n",
    "def load_shapefile_safe(filepath):\n",
    "    try:\n",
    "        gdf = gpd.read_file(filepath)\n",
    "        print(f\"‚úÖ Loaded shapefile: {filepath.name} with {len(gdf)} records\")\n",
    "        return gdf\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Shapefile not found: {filepath}\")\n",
    "        return gpd.GeoDataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading shapefile {filepath.name}: {e}\")\n",
    "        return gpd.GeoDataFrame()\n",
    "\n",
    "# Load datasets\n",
    "climate_df = load_csv_safe(data_path / \"climate_data_nepal_district_wise_daily_1981_2019.csv.gz\")\n",
    "glacier_df = load_csv_safe(data_path / \"glaciers_change_in_basins_subbasins_1980_1990_2000_2010.csv\")\n",
    "land_use_df = load_csv_safe(data_path / \"land_use_statistics_1967_2010.csv\")\n",
    "agri_df = load_csv_safe(data_path / \"nepal_agri_stats_cereal_197980_201314.csv\")\n",
    "local_units_gdf = load_shapefile_safe(data_path / \"local_unit_shapefiles\" / \"local_unit.shp\")\n",
    "\n",
    "# Display basic summaries\n",
    "datasets = {\n",
    "    \"Climate Data\": climate_df,\n",
    "    \"Glacier Change Data\": glacier_df,\n",
    "    \"Land Use Statistics\": land_use_df,\n",
    "    \"Agricultural Statistics\": agri_df,\n",
    "    \"Geospatial Data (Local Units)\": local_units_gdf\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    if df.empty:\n",
    "        print(\"‚ö†Ô∏è Dataset is empty or failed to load.\\n\")\n",
    "    else:\n",
    "        df.info()\n",
    "        print(df.head(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64677680-a817-4010-b7d4-6668b83b7ca2",
   "metadata": {},
   "source": [
    "### ‚úÖ Climate Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fea978-9f19-4adc-bfba-77b5a3f4c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "climate_path = '../data/climate_data_nepal_district_wise_daily_1981_2019.csv.gz'\n",
    "climate_df = pd.read_csv(climate_path)\n",
    "\n",
    "# Convert DATE to datetime safely\n",
    "climate_df['DATE'] = pd.to_datetime(climate_df['DATE'], format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# Extract year, month, and day from DATE\n",
    "climate_df['Year'] = climate_df['DATE'].dt.year\n",
    "climate_df['Month'] = climate_df['DATE'].dt.month\n",
    "climate_df['Day'] = climate_df['DATE'].dt.day\n",
    "\n",
    "# Define Nepal's meteorological seasons\n",
    "def get_season(month):\n",
    "    if pd.isna(month):\n",
    "        return 'Unknown'\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Autumn'\n",
    "    elif month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    return 'Unknown'\n",
    "\n",
    "climate_df['Season'] = climate_df['Month'].apply(get_season)\n",
    "\n",
    "# Report missing values\n",
    "missing = climate_df.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "if not missing.empty:\n",
    "    print(\"üîç Missing Values:\\n\", missing)\n",
    "else:\n",
    "    print(\"‚úÖ No missing values detected.\")\n",
    "\n",
    "# Display summary statistics for numeric columns\n",
    "print(\"\\nüìä Summary Statistics:\")\n",
    "print(climate_df.describe(include='number'))\n",
    "\n",
    "# Preview the updated DataFrame\n",
    "print(\"\\nüìã Updated DataFrame Preview:\")\n",
    "print(climate_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ada641-dd2a-4f88-a4e6-27495b679211",
   "metadata": {},
   "source": [
    "### ‚úÖ Glacier Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf38ab4-ca40-4431-9052-35bd5c0ca971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load glacier data\n",
    "glacier_path = '../data/glaciers_change_in_basins_subbasins_1980_1990_2000_2010.csv'\n",
    "glacier_df = pd.read_csv(glacier_path)\n",
    "\n",
    "# Step 2: Standardize column names\n",
    "glacier_df.columns = (\n",
    "    glacier_df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace('~', '', regex=False)\n",
    "    .str.replace(' ', '_')\n",
    "    .str.replace(r'\\(km2\\)', '', regex=True)\n",
    "    .str.replace(r'\\(km3\\)', '', regex=True)\n",
    "    .str.replace(r'\\(masl\\)', '', regex=True)\n",
    "    .str.replace(r'[()]', '', regex=True)\n",
    ")\n",
    "\n",
    "# Step 3: Rename columns for reshaping\n",
    "glacier_df.rename(columns={\n",
    "    'glacier_no._in_1980': 'glacier_count_1980',\n",
    "    'glacier_no._in_1990': 'glacier_count_1990',\n",
    "    'glacier_no._in_2000': 'glacier_count_2000',\n",
    "    'glacier_no._in_2010': 'glacier_count_2010',\n",
    "    'glacier_area_in_1980': 'glacier_area_1980',\n",
    "    'glacier_area_1990': 'glacier_area_1990',\n",
    "    'glacier_area_2000': 'glacier_area_2000',\n",
    "    'glacier_area_2010': 'glacier_area_2010',\n",
    "    'estimated_ice_reserved_1980': 'ice_volume_1980',\n",
    "    'estimated_ice_reserved_1990': 'ice_volume_1990',\n",
    "    'estimated_ice_reserved2000': 'ice_volume_2000',\n",
    "    'estimated_ice_reserved2010': 'ice_volume_2010',\n",
    "    'minimum_elevation_in_1980': 'min_elev_1980',\n",
    "    'minimum_elevation_in1990': 'min_elev_1990',\n",
    "    'minimum_elevation_in2000': 'min_elev_2000',\n",
    "    'minimum_elevation_in2010': 'min_elev_2010'\n",
    "}, inplace=True)\n",
    "\n",
    "# Step 4: Reshape from wide to long format\n",
    "glacier_long = pd.wide_to_long(\n",
    "    glacier_df,\n",
    "    stubnames=['glacier_count', 'glacier_area', 'ice_volume', 'min_elev'],\n",
    "    i=['basin', 'sub-basin'],\n",
    "    j='year',\n",
    "    sep='_',\n",
    "    suffix='(1980|1990|2000|2010)'\n",
    ").reset_index()\n",
    "\n",
    "# Step 5: Convert year to integer\n",
    "glacier_long['year'] = glacier_long['year'].astype(int)\n",
    "\n",
    "# Step 6: Preview final output\n",
    "print(\"‚úÖ Glacier Data (Long Format with Year Column):\")\n",
    "print(glacier_long.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b0ef60-e34b-461a-ac57-d18c7b9e9a51",
   "metadata": {},
   "source": [
    "### ‚úÖ Glacier change metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a6cc33-3ee6-446e-9a3b-816b5ebcfd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Pivot glacier_long to compare 1980 vs 2010\n",
    "pivoted = glacier_long.pivot_table(\n",
    "    index=['basin', 'sub-basin'],\n",
    "    columns='year',\n",
    "    values=['glacier_area', 'ice_volume', 'min_elev']\n",
    ").reset_index()\n",
    "\n",
    "# Step 2: Flatten multi-level column headers\n",
    "pivoted.columns = [\n",
    "    f\"{var}_{int(year)}\" if isinstance(year, (int, float)) else var\n",
    "    for var, year in pivoted.columns.to_flat_index()\n",
    "]\n",
    "\n",
    "# Step 3: Calculate absolute changes between 1980 and 2010\n",
    "pivoted['area_change_1980_2010'] = pivoted['glacier_area_2010'] - pivoted['glacier_area_1980']\n",
    "pivoted['ice_loss_km3'] = pivoted['ice_volume_1980'] - pivoted['ice_volume_2010']\n",
    "pivoted['elev_rise_m'] = pivoted['min_elev_2010'] - pivoted['min_elev_1980']\n",
    "\n",
    "# Step 4: Preview key metrics\n",
    "print(\"‚úÖ Glacier Change Indicators (1980‚Äì2010):\")\n",
    "print(\n",
    "    pivoted[\n",
    "        ['basin', 'sub-basin', 'area_change_1980_2010', 'ice_loss_km3', 'elev_rise_m']\n",
    "    ].round(2).head()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf4deb-3c6e-403c-9a1f-baac8588b4fe",
   "metadata": {},
   "source": [
    "### ‚úÖ Land Use Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b51ed1-7c8e-4614-b94c-cc01fb6ea1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load land use data\n",
    "land_use_path = '../data/land_use_statistics_1967_2010.csv'\n",
    "land_use_df = pd.read_csv(land_use_path)\n",
    "\n",
    "# Step 2: Clean column names\n",
    "land_use_df.columns = (\n",
    "    land_use_df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(' ', '_')\n",
    "    .str.replace('%', '', regex=False)  # Remove '%' sign\n",
    ")\n",
    "\n",
    "# Step 3: Display structure and preview\n",
    "print(\"üìÑ Land Use Data Info:\")\n",
    "land_use_df.info()\n",
    "print(\"\\nüîç Land Use Data Preview:\")\n",
    "print(land_use_df.head())\n",
    "\n",
    "# Step 4: Melt to long format\n",
    "land_use_long = land_use_df.melt(\n",
    "    id_vars='land_use_type',\n",
    "    var_name='year',\n",
    "    value_name='percentage'\n",
    ")\n",
    "\n",
    "# Step 5: Extract numeric year from column names\n",
    "land_use_long['year'] = pd.to_numeric(\n",
    "    land_use_long['year'].str.extract(r'(\\d{4})')[0],\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Step 6: Standardize land use type names\n",
    "land_use_long['land_use_type'] = (\n",
    "    land_use_long['land_use_type']\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(r'[^a-z0-9_]+', '', regex=True)  # Remove non-alphanum (e.g., '*')\n",
    ")\n",
    "\n",
    "# Step 7: Drop missing values\n",
    "land_use_long.dropna(subset=['year', 'percentage'], inplace=True)\n",
    "\n",
    "# Step 8: Preview final dataset\n",
    "print(\"\\nüìä Tidy Land Use Data (Long Format):\")\n",
    "print(land_use_long.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a2dccc-5f11-403d-80b6-355ebe7311a6",
   "metadata": {},
   "source": [
    "### Land use trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c9c3ac-2420-40fb-a54d-ea8b1283fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=land_use_long, x='year', y='percentage', hue='land_use_type', marker='o')\n",
    "plt.title(\"Land Use Change in Nepal (1967‚Äì2010)\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Percentage of Land\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb19f9a0-e732-46c5-8291-39a2012c26fe",
   "metadata": {},
   "source": [
    "### ‚úÖ Cereal Yield Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56a8cd2-73e1-4e04-941c-f64ff7cb5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "agri_path = \"../data/nepal_agri_stats_cereal_197980_201314.csv\"\n",
    "agri_df = pd.read_csv(agri_path)\n",
    "\n",
    "# Overview\n",
    "print(\"üìÑ Cereal Yield Data Info:\")\n",
    "agri_df.info()\n",
    "print(\"\\nüîç Preview:\")\n",
    "print(agri_df.head())\n",
    "\n",
    "# Step 1: Clean column names\n",
    "agri_df.columns = agri_df.columns.str.strip().str.upper()\n",
    "\n",
    "# Step 2: Identify yield columns (those with '_Y_')\n",
    "yield_cols = [col for col in agri_df.columns if '_Y_' in col]\n",
    "\n",
    "# Step 3: Subset dataframe\n",
    "yield_df = agri_df[['DISTRICT_NAME'] + yield_cols].copy()\n",
    "\n",
    "# Step 4: Melt to long format\n",
    "yield_long = yield_df.melt(\n",
    "    id_vars='DISTRICT_NAME',\n",
    "    var_name='CROP_FY',\n",
    "    value_name='YIELD'\n",
    ")\n",
    "\n",
    "# Step 5: Extract crop and fiscal year\n",
    "extracted = yield_long['CROP_FY'].str.extract(r'([A-Z]+)_Y_(\\d{6})')\n",
    "yield_long['CROP'] = extracted[0].str.title()  # Capitalize crop names\n",
    "yield_long['FY'] = extracted[1]\n",
    "\n",
    "# Step 6: Format FY (e.g., \"197980\" ‚Üí \"1979/80\")\n",
    "yield_long['FY'] = yield_long['FY'].apply(lambda x: f\"{x[:4]}/{x[4:]}\" if pd.notna(x) else None)\n",
    "\n",
    "# Step 7: Drop missing values\n",
    "yield_long.dropna(subset=['DISTRICT_NAME', 'CROP', 'FY', 'YIELD'], inplace=True)\n",
    "\n",
    "# Step 8: Final tidy DataFrame\n",
    "yield_long = yield_long[['DISTRICT_NAME', 'CROP', 'FY', 'YIELD']]\n",
    "\n",
    "# Preview tidy result\n",
    "print(\"\\n‚úÖ Tidy Yield Data Preview:\")\n",
    "print(yield_long.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ddf2f9-6451-4170-a042-087cefad85df",
   "metadata": {},
   "source": [
    "### Yield Trends Over Time by Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e604d655-bd6d-41c9-9d23-d24a0931e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=yield_long, x='FY', y='YIELD', hue='CROP', estimator='mean', marker='o')\n",
    "plt.title(\"Average Cereal Yield Over Time by Crop\")\n",
    "plt.xlabel(\"Fiscal Year\")\n",
    "plt.ylabel(\"Yield (kg/ha)\")\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345eab5-4612-4bd9-b54b-b7607e1b1511",
   "metadata": {},
   "source": [
    "### ‚úÖ Geospatial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31085c-377b-4972-a769-a0b35ce96d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "# Step 1: Load the shapefile\n",
    "data_path = Path(\"../data\") / \"local_unit_shapefiles\" / \"local_unit.shp\"\n",
    "gdf = gpd.read_file(data_path)\n",
    "\n",
    "# Step 2: Normalize column names\n",
    "gdf.columns = gdf.columns.str.strip().str.upper()\n",
    "\n",
    "# Step 3: Standardize district names\n",
    "gdf['DISTRICT_NAME'] = gdf['DISTRICT'].str.strip().str.lower()\n",
    "\n",
    "# ‚úÖ Step 4: Ensure 'GEOMETRY' is set as active geometry\n",
    "if 'GEOMETRY' in gdf.columns:\n",
    "    gdf = gdf.set_geometry('GEOMETRY')\n",
    "\n",
    "# Step 5: Dissolve polygons to one per district\n",
    "district_gdf = gdf.dissolve(by='DISTRICT_NAME', as_index=False)\n",
    "\n",
    "# Step 6: Project to UTM Zone 45N (EPSG:32645) for spatial accuracy\n",
    "district_gdf_proj = district_gdf.to_crs(epsg=32645)\n",
    "\n",
    "# Step 7: Compute centroids (in projected CRS)\n",
    "district_gdf_proj['CENTROID'] = district_gdf_proj.geometry.centroid\n",
    "\n",
    "# Step 8: Convert centroids to WGS84 for lat/lon extraction\n",
    "centroids_wgs84 = district_gdf_proj.set_geometry('CENTROID').to_crs(epsg=4326)\n",
    "centroids_wgs84['CENTROID_LAT'] = centroids_wgs84.geometry.y\n",
    "centroids_wgs84['CENTROID_LON'] = centroids_wgs84.geometry.x\n",
    "\n",
    "# Step 9: Final output\n",
    "district_centroids = centroids_wgs84[['DISTRICT_NAME', 'CENTROID_LAT', 'CENTROID_LON']]\n",
    "\n",
    "# Step 10: Preview\n",
    "print(\"‚úÖ District Centroid Coordinates:\")\n",
    "print(district_centroids.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

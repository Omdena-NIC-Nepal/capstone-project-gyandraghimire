# pages/nlp_page.py

import streamlit as st
import pandas as pd
import docx
from nlp_utils import run_nlp_pipeline

def render():
    st.title("ğŸ—ï¸ NLP Dashboard: Climate News Analysis")
    st.markdown("""
        Analyze climate-related documents or news using **advanced NLP tools**:  
        - ğŸ’¬ Sentiment Analysis  
        - ğŸ§  Topic Modeling  
        - ğŸ” Named Entity Recognition  
        - ğŸ“ Summarization  
    """)

    # --- Input Type Selection ---
    st.subheader("ğŸ” Select Input Method")
    input_mode = st.radio("Choose input format:", ["ğŸŒ URLs", "ğŸ–‹ï¸ Raw Text", "ğŸ“ Upload File"])

    texts, urls = [], []

    # --- Handle Each Input Mode ---
    if input_mode == "ğŸŒ URLs":
        st.info("Paste one article URL per line:")
        default_urls = "\n".join([
            "https://risingnepaldaily.com/news/59885",
            "https://risingnepaldaily.com/news/52538",
            "https://risingnepaldaily.com/news/51726",
            "https://risingnepaldaily.com/news/52739",
            "https://risingnepaldaily.com/news/55425",
            "https://risingnepaldaily.com/news/41304"
        ])
        url_input = st.text_area("ğŸ”— Article URLs", default_urls, height=150)
        urls = [u.strip() for u in url_input.splitlines() if u.strip()]

    elif input_mode == "ğŸ–‹ï¸ Raw Text":
        st.info("Paste or type any custom text you want to analyze:")
        raw_text = st.text_area("ğŸ–Šï¸ Text Input", height=300)
        if raw_text.strip():
            texts = [raw_text.strip()]

    elif input_mode == "ğŸ“ Upload File":
        st.info("Upload a `.txt`, `.csv`, or `.docx` file for NLP analysis:")
        file = st.file_uploader("ğŸ“‚ Choose File", type=["txt", "csv", "docx"])
        if file:
            try:
                if file.name.endswith(".txt"):
                    texts = [file.read().decode("utf-8")]
                elif file.name.endswith(".csv"):
                    df = pd.read_csv(file)
                    texts = df.iloc[:, 0].dropna().astype(str).tolist()
                elif file.name.endswith(".docx"):
                    doc = docx.Document(file)
                    texts = ['\n'.join(p.text for p in doc.paragraphs)]
                else:
                    st.error("Unsupported file type.")
                st.success(f"âœ… Loaded content from `{file.name}`")
            except Exception as e:
                st.error(f"âŒ Could not read file: {e}")

    # --- Run NLP Analysis ---
    if st.button("ğŸ” Run NLP Analysis"):
        if not (texts or urls):
            st.warning("âš ï¸ Please provide some input to analyze.")
            return

        try:
            with st.spinner("ğŸ”„ Running NLP pipeline..."):
                results = run_nlp_pipeline(urls=urls if urls else None, texts=texts if texts else None)
            st.success("âœ… NLP Analysis Complete")
        except Exception as e:
            st.error(f"âŒ Error during analysis: {e}")
            return

        # --- Summaries ---
        st.subheader("ğŸ“ Text Summaries")
        for i, summary in enumerate(results.get("summaries", [])):
            with st.expander(f"ğŸ“„ Summary {i + 1}", expanded=False):
                st.markdown(summary)

        # --- Topics ---
        st.subheader("ğŸ“š Extracted Topics")
        for topic in results.get("topics", []):
            st.markdown(f"- {topic}")

        # --- Sentiment ---
        st.subheader("ğŸ’¬ Sentiment Analysis")
        for i, sentiment in enumerate(results.get("sentiments", [])):
            label = sentiment.get("label", "Unknown").upper()
            score = sentiment.get("score", 0.0)
            badge = "ğŸŸ¢ Positive" if label == "POSITIVE" else "ğŸ”´ Negative" if label == "NEGATIVE" else "âšª Neutral"
            st.markdown(f"**Document {i + 1}:** {badge} (Confidence: {score:.2f})")

        # --- Entities ---
        st.subheader("ğŸ” Named Entity Recognition")
        for i, entities in enumerate(results.get("entities", [])):
            with st.expander(f"ğŸ§¾ Entities in Document {i + 1}", expanded=False):
                if not entities:
                    st.info("No named entities detected.")
                else:
                    for ent in entities:
                        st.markdown(f"- `{ent['entity_group']}` â†’ **{ent['word']}**")
